{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Backtranslation Notebook to Rule Over Them All\n",
    "Please do ignore the title, I just wanted to make it sound cool. This notebook is a simple demonstration of how to use backtranslation to improve the performance of a machine learning model. The notebook is divided into the following sections:\n",
    "1. Introduction and Simple Generation\n",
    "2. Applying Data Preparation/Filtering\n",
    "3. Investigating Iterative Backtranslation\n",
    "4. Applying a similar pipeline to a more complex model (ALMA-R)\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cell in the notebook to enable autoreload of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-de-en and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 273,027,072 || trainable%: 0.4320626490841172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
    "from utils.data import TranslationDataModule\n",
    "from utils.models import TranslationLightning\n",
    "from pytorch_lightning import Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "# Isn't it so nice and clean now? I went through FOUR different ways of doing this before I thought of this one. WDWFDGFEQWDQWFGA\n",
    "\n",
    "SRC = \"de\"\n",
    "TGT = \"en\"\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "cwd = Path.cwd()\n",
    "data_dir = cwd / \"Data\"\n",
    "\n",
    "it_parallel = \"it-parallel\"\n",
    "news_dataset = \"train-euro-news-big\"\n",
    "it_mono = \"it-mono\"\n",
    "\n",
    "test_folder = cwd / \"tests\"\n",
    "\n",
    "mname = f\"facebook/wmt19-{SRC}-{TGT}\"\n",
    "tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
    "model = FSMTForConditionalGeneration.from_pretrained(mname)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 16,\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.2,\n",
    "    target_modules = [\"v_proj\", \"q_proj\"]\n",
    ")\n",
    "\n",
    "it_parallel_data = TranslationDataModule(data_dir / it_parallel, SRC, TGT, tokenizer, model, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "# Add news_dataset\n",
    "\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model_pl = TranslationLightning(model, tokenizer, lr=3e-4, adam_beta=(0.9, 0.98), weight_decay=1e-4, test_folder = test_folder)\n",
    "\n",
    "trainer = Trainer(max_steps=10,  gradient_clip_val=0.1, val_check_interval = 0.25, limit_val_batches=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_parallel_data.setup('fit')\n",
    "dataset = it_parallel_data.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': tensor([[26298,  3875,    93,  2381,   144,    95,  2217,   125,   504,     4,\n",
      "           319,    13,  3296,   125,   368,    10, 27477,   183,    10,  5040,\n",
      "          2220,  3966,  2567,  8789,  1871,    12,   308,  3689,    83,   101,\n",
      "            17,  8627,   205,    28,    13,    59,  1606,  3746,     5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[13484,  1697,    16,  3875,  9989,   485,    94,    24,   144,    95,\n",
      "          2217,   125,   504,    24,     4,   262,  3296,   125,   368,    10,\n",
      "         27477,   183,  5040,  2220,  3849, 21794,    10,    11,    10, 10478,\n",
      "             4,    41,   387,    41,  4864,  1562,   427,     5,     2]])}, {'input_ids': tensor([[3257,  730,  948,   20, 4162, 9859,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[8882, 1089,   20, 4162,  325,  536,    2]])}, {'input_ids': tensor([[5629,   21, 4085, 3347,   81,    8,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[4085, 3347,   81, 2864,    8,    2]])}, {'input_ids': tensor([[ 8907,    15,  4057,    81, 24087,   559,   182,   948,    14,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  68, 1675, 1549,   15, 2277, 1698,   81, 2781,  559,   83,   14,    2]])}, {'input_ids': tensor([[2455,  258,    2]]), 'attention_mask': tensor([[1, 1, 1]]), 'labels': tensor([[1653,  258,    2]])}, {'input_ids': tensor([[   97,    25,  1163,    12,  8477,   105,   148,  4801,   125,  9497,\n",
      "            54,  9051,    21,  4688,   196,  6427,   279,    81,   203,  4830,\n",
      "            12,   155,   226,   196,  6427,   279,    81,   203,   308, 13552,\n",
      "          2509,     5,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  142,     5,  3276,    20,  5969,  4599,   355,   222,  8018,  1145,\n",
      "           203,   670,    20,  2493,   203,  7973, 12917,     2]])}, {'input_ids': tensor([[ 118, 1065, 5892, 3102,  320, 7101, 1574,   91, 9097,   74,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  118,  1065,  2704,  9271, 13240,   485,   183,   897,  5919,     2]])}, {'input_ids': tensor([[8561,   20, 3521,    8,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]]), 'labels': tensor([[1170,   20,   76, 1074,    8,    2]])}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not BatchEncoding",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit_parallel_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project/nlp2/utils/data.py:56\u001b[0m, in \u001b[0;36mTranslationDataModule.collate_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m---> 56\u001b[0m         batch[key] \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollator(batch)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not BatchEncoding"
     ]
    }
   ],
   "source": [
    "batch = next(iter(it_parallel_data.train_dataloader()))\n",
    "print(batch['input_ids'].shape, batch['labels'].shape, batch['decoder_input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1124, 8001,   19, 1647,   38,   22,  675,    7,    6, 1775, 6221,   48,\n",
       "            6, 1867,    4,  399, 1203, 3263,  119,   99, 7555,    9,    6, 1647,\n",
       "            7,    6, 6221,    4,   11,  755, 8001,   61,   48,    6, 1867,    5,\n",
       "            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1]),\n",
       " tensor([   2, 1124, 8001,   19, 1647,   38,   22,  675,    7,    6, 1775, 6221,\n",
       "           48,    6, 1867,    4,  399, 1203, 3263,  119,   99, 7555,    9,    6,\n",
       "         1647,    7,    6, 6221,    4,   11,  755, 8001,   61,   48,    6, 1867,\n",
       "            5,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "\n",
    "def prep_fn(sample):\n",
    "    sample['input_ids'] = sample['input_ids'].squeeze()\n",
    "    sample['attention_mask'] = sample['attention_mask'].squeeze()\n",
    "    sample['labels'] = sample['labels'].squeeze()\n",
    "    return sample\n",
    "this = [prep_fn(dataset[i]) for i in range(8)]\n",
    "\n",
    "# print(this)\n",
    "batch = data_collator(this)\n",
    "batch.labels[1], batch.decoder_input_ids[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_parallel_data.setup('fit')\n",
    "# batch = next(iter(it_parallel_data.train_dataloader()))\n",
    "\n",
    "# print(batch[\"labels\"].)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Generation\n",
    "Here we simply load a pre-trained model and run inference on the test set. The model loaded is a pre-trained model from the [Hugging Face Transformers](https://huggingface.co/transformers/) library. \n",
    "\n",
    "TODO: Evaluate the model on the NEWS set and report the BLEU score.\n",
    "\n",
    "| Model | it-parallel | NEWS |\n",
    "| --- | --- | --- |\n",
    "| Base | 38.307622648987454 | 0.0 |\n",
    "| FT on IT-parallel | 29.919114415962934 | 0.0 |\n",
    "| FT on BT | 29.919114415962934 | 0.0 |\n",
    "| FT on IT-parallel and BT | 29.919114415962934 | 0.0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT-parallel\n",
    "results = trainer.predict(model_pl, datamodule=it_parallel_data)\n",
    "bleu = sum(r[2][\"score\"] for r in results) / len(results)\n",
    "print(\"Average BLEU:\", bleu)\n",
    "\n",
    "# News corpus\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can train the model\n",
    "trainer.fit(model_pl, datamodule=it_parallel_data)\n",
    "\n",
    "# IT-parallel\n",
    "trainer.predict(model_pl, datamodule=it_parallel_data)\n",
    "bleu = sum(r[2][\"score\"] for r in results) / len(results)\n",
    "print(\"Average BLEU:\", bleu)\n",
    "\n",
    "# News corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the key ingredient: Backtranslation\n",
    "Backtranslation uses a reverse model to generate synthetic data. This synthetic data is then used to train the model. The idea is that the synthetic data will help the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_model = FSMTForConditionalGeneration.from_pretrained(f\"facebook/wmt19-{TGT}-{SRC}\")\n",
    "reverse_tokenizer = FSMTTokenizer.from_pretrained(f\"facebook/wmt19-{TGT}-{SRC}\")\n",
    "\n",
    "it_mono = TranslationDataModule(data_dir / it_mono, TGT, SRC, reverse_tokenizer, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "\n",
    "output_folder = test_folder / f\"generation-{TGT}-{SRC}\"\n",
    "reverse_lightning = TranslationLightning(reverse_model, reverse_tokenizer, lr=3e-4, adam_beta=(0.9, 0.98), weight_decay=1e-4, test_folder = output_folder)\n",
    "\n",
    "trainer.predict(reverse_lightning, it_mono.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated the data, we can now copy in the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = output_folder / \"hypothesis.hyp\"\n",
    "train_file_target = output_folder / \"source.src\"\n",
    "train_file.rename(data_dir / f\"generated-{it_mono}\" / f\"train.{SRC}\")\n",
    "train_file_target.rename(data_dir / f\"generated-{it_mono}\" / f\"train.{TGT}\")\n",
    "\n",
    "import shutil\n",
    "bt_dir = data_dir / f\"generated-{it_mono}\"\n",
    "shutil.copy(data_dir / it_mono / f\"dev.{SRC}\", bt_dir / f\"dev.{SRC}\")\n",
    "shutil.copy(data_dir / it_mono / f\"dev.{TGT}\", bt_dir / f\"dev.{TGT}\")\n",
    "shutil.copy(data_dir / it_mono / f\"test.{SRC}\", bt_dir / f\"test.{SRC}\")\n",
    "shutil.copy(data_dir / it_mono / f\"test.{TGT}\", bt_dir / f\"test.{TGT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we can finally train...\n",
    "it_mono_bt = TranslationDataModule(data_dir / f\"generated-{it_mono}\", SRC, TGT, tokenizer, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "trainer.fit(model_pl, datamodule=it_mono_bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pl.test_folder = test_folder / f\"bt_{it_mono}\"\n",
    "# IT-parallel\n",
    "results = trainer.predict(model_pl, datamodule=it_parallel_data)\n",
    "print(\"IT-parallel\", sum(r[2][\"score\"] for r in results) / len(results))\n",
    "# News corpus\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now finally we can concatenate both the original and the generated data and train the model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_plus_bt = data_dir / f\"parallel+bt_generated_{it_mono}\"\n",
    "shutil.copytree(bt_dir, parallel_plus_bt)\n",
    "train_file = parallel_plus_bt /f\"train.{SRC}\"\n",
    "with open(train_file, \"r+\") as f:\n",
    "    with open(data_dir / it_parallel / f\"train.{SRC}\", \"r\") as f2:\n",
    "        f.write(\"\\n\" + f2.read())\n",
    "        \n",
    "train_file_target = parallel_plus_bt / f\"train.{TGT}\"\n",
    "with open(train_file_target, \"r+\") as f:\n",
    "    with open(data_dir / it_parallel / f\"train.{TGT}\", \"r\") as f2:\n",
    "        f.write(\"\\n\" + f2.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model on the combined data\n",
    "parallel_plus_bt_data = TranslationDataModule(parallel_plus_bt, SRC, TGT, tokenizer, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "trainer.fit(model_pl, datamodule=parallel_plus_bt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT-parallel\n",
    "results = trainer.predict(model_pl, datamodule=it_parallel_data)\n",
    "print(\"IT-parallel BLEU:\", sum(r[2][\"score\"] for r in results) / len(results))\n",
    "# News corpus\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
