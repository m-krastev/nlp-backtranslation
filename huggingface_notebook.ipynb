{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Backtranslation Notebook to Rule Over Them All\n",
    "Please do ignore the title, I just wanted to make it sound cool. This notebook is a simple demonstration of how to use backtranslation to improve the performance of a machine learning model. The notebook is divided into the following sections:\n",
    "1. Introduction and Simple Generation\n",
    "2. Applying Data Preparation/Filtering\n",
    "3. Investigating Iterative Backtranslation\n",
    "4. Applying a similar pipeline to a more complex model (ALMA-R)\n",
    "5. Conclusion\n",
    "\n",
    "Results:\n",
    "\n",
    "| Model | IT |  | NEWS | |\n",
    "| ---   |---- | --- | --- | --- |\n",
    "|       | BLEU| COMET|BLEU|COMET|\n",
    "| Base | 38.31 (0.819±.145) | 29.226±7.710 (0.834±0.077) |\n",
    "| FT on IT-parallel | 38.92 (0.825±.145) | 28.646 (0.827±0.064) |\n",
    "| FT on BT | 41.827 (0.84±.0) | 27.115±7.197 (0.820±0.084) |\n",
    "| FT on IT-parallel and BT | 39.879 (0.86±.1) | 26.655±7.249 (0.819±0.084) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cell in the notebook to enable autoreload of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-de-en and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 273,027,072 || trainable%: 0.4320626490841172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
    "from utils.data import TranslationDataModule\n",
    "from utils.models import TranslationLightning\n",
    "from pytorch_lightning import Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "# Isn't it so nice and clean now? I went through FOUR different ways of doing this before I thought of this one. WDWFDGFEQWDQWFGA\n",
    "\n",
    "SRC = \"de\"\n",
    "TGT = \"en\"\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "cwd = Path.cwd()\n",
    "data_dir = cwd / \"Data\"\n",
    "\n",
    "it_parallel = \"it-parallel\"\n",
    "news_dataset = \"train-euro-news-big\"\n",
    "it_mono = \"it-mono\"\n",
    "\n",
    "test_folder = cwd / \"tests\"\n",
    "\n",
    "mname = f\"facebook/wmt19-{SRC}-{TGT}\"\n",
    "tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
    "model = FSMTForConditionalGeneration.from_pretrained(mname)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 16,\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.2,\n",
    "    target_modules = [\"v_proj\", \"q_proj\"]\n",
    ")\n",
    "\n",
    "it_parallel_data = TranslationDataModule(data_dir / it_parallel, SRC, TGT, tokenizer, None, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "# Add news_dataset\n",
    "\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model_pl = TranslationLightning(model, tokenizer, lr=3e-4, adam_beta=(0.9, 0.98), weight_decay=1e-4, test_folder = test_folder)\n",
    "\n",
    "trainer = Trainer(max_steps=10,  gradient_clip_val=0.1, val_check_interval = 0.25, limit_val_batches=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_parallel_data = TranslationDataModule(data_dir / it_parallel, SRC, TGT, tokenizer, model, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "\n",
    "it_parallel_data.setup('fit')\n",
    "dataset = it_parallel_data.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(it_parallel_data.test_dataloader()))\n",
    "# batch['labels'][batch['labels']==1] = -100\n",
    "# batch.pop('decoder_input_ids')\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9212, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pl.training_step(batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_parallel_data.setup('fit')\n",
    "# batch = next(iter(it_parallel_data.train_dataloader()))\n",
    "\n",
    "# print(batch[\"labels\"].)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Generation\n",
    "Here we simply load a pre-trained model and run inference on the test set. The model loaded is a pre-trained model from the [Hugging Face Transformers](https://huggingface.co/transformers/) library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT-parallel\n",
    "results = trainer.predict(model_pl, datamodule=it_parallel_data)\n",
    "bleu = sum(r[2][\"score\"] for r in results) / len(results)\n",
    "print(\"Average BLEU:\", bleu)\n",
    "\n",
    "# News corpus\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can train the model\n",
    "trainer.fit(model_pl, datamodule=it_parallel_data)\n",
    "\n",
    "# IT-parallel\n",
    "trainer.predict(model_pl, datamodule=it_parallel_data)\n",
    "bleu = sum(r[2][\"score\"] for r in results) / len(results)\n",
    "print(\"Average BLEU:\", bleu)\n",
    "\n",
    "# News corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the key ingredient: Backtranslation\n",
    "Backtranslation uses a reverse model to generate synthetic data. This synthetic data is then used to train the model. The idea is that the synthetic data will help the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_model = FSMTForConditionalGeneration.from_pretrained(f\"facebook/wmt19-{TGT}-{SRC}\")\n",
    "reverse_tokenizer = FSMTTokenizer.from_pretrained(f\"facebook/wmt19-{TGT}-{SRC}\")\n",
    "\n",
    "it_mono = TranslationDataModule(data_dir / it_mono, TGT, SRC, reverse_tokenizer, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "\n",
    "output_folder = test_folder / f\"generation-{TGT}-{SRC}\"\n",
    "reverse_lightning = TranslationLightning(reverse_model, reverse_tokenizer, lr=3e-4, adam_beta=(0.9, 0.98), weight_decay=1e-4, test_folder = output_folder)\n",
    "\n",
    "trainer.predict(reverse_lightning, it_mono.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated the data, we can now copy in the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = output_folder / \"hypothesis.hyp\"\n",
    "train_file_target = output_folder / \"source.src\"\n",
    "train_file.rename(data_dir / f\"generated-{it_mono}\" / f\"train.{SRC}\")\n",
    "train_file_target.rename(data_dir / f\"generated-{it_mono}\" / f\"train.{TGT}\")\n",
    "\n",
    "import shutil\n",
    "bt_dir = data_dir / f\"generated-{it_mono}\"\n",
    "shutil.copy(data_dir / it_mono / f\"dev.{SRC}\", bt_dir / f\"dev.{SRC}\")\n",
    "shutil.copy(data_dir / it_mono / f\"dev.{TGT}\", bt_dir / f\"dev.{TGT}\")\n",
    "shutil.copy(data_dir / it_mono / f\"test.{SRC}\", bt_dir / f\"test.{SRC}\")\n",
    "shutil.copy(data_dir / it_mono / f\"test.{TGT}\", bt_dir / f\"test.{TGT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we can finally train...\n",
    "it_mono_bt = TranslationDataModule(data_dir / f\"generated-{it_mono}\", SRC, TGT, tokenizer, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "trainer.fit(model_pl, datamodule=it_mono_bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pl.test_folder = test_folder / f\"bt_{it_mono}\"\n",
    "# IT-parallel\n",
    "results = trainer.predict(model_pl, datamodule=it_parallel_data)\n",
    "print(\"IT-parallel\", sum(r[2][\"score\"] for r in results) / len(results))\n",
    "# News corpus\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now finally we can concatenate both the original and the generated data and train the model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_plus_bt = data_dir / f\"parallel+bt_generated_{it_mono}\"\n",
    "shutil.copytree(bt_dir, parallel_plus_bt)\n",
    "train_file = parallel_plus_bt /f\"train.{SRC}\"\n",
    "with open(train_file, \"r+\") as f:\n",
    "    with open(data_dir / it_parallel / f\"train.{SRC}\", \"r\") as f2:\n",
    "        f.write(\"\\n\" + f2.read())\n",
    "        \n",
    "train_file_target = parallel_plus_bt / f\"train.{TGT}\"\n",
    "with open(train_file_target, \"r+\") as f:\n",
    "    with open(data_dir / it_parallel / f\"train.{TGT}\", \"r\") as f2:\n",
    "        f.write(\"\\n\" + f2.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model on the combined data\n",
    "parallel_plus_bt_data = TranslationDataModule(parallel_plus_bt, SRC, TGT, tokenizer, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "trainer.fit(model_pl, datamodule=parallel_plus_bt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT-parallel\n",
    "results = trainer.predict(model_pl, datamodule=it_parallel_data)\n",
    "print(\"IT-parallel BLEU:\", sum(r[2][\"score\"] for r in results) / len(results))\n",
    "# News corpus\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Investigating Iterative Backtranslation\n",
    "The basic idea, proposed by Hoang et al. (2018), is to iteratively backtranslate the data. For our purposes, we require two separate models, one for forward translation (EN-DE) and one for reverse translation (DE-EN). We can then use them in turns, first generating synthetic data, training the model, and then using the trained forward model to generate synthetic data for the reverse model. This process may be iterated until convergence.\n",
    "\n",
    "We can combine this approach with concatenating the high quality parallel data to improve the performance of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we might want to fine-tune the model on the IT-parallel data\n",
    "!python train.py \\\n",
    "    --output_dir tests/reverse-it-parallel-de-en \\\n",
    "    --srclang de --tgtlang en \\\n",
    "    --batch_size 32 \\\n",
    "    --max_length 128 \\\n",
    "    --epochs 15 \\\n",
    "    --train_dir it-parallel --seed 42 \\\n",
    "    --check_val_every_n_epoch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can generate translations for the IT-mono data\n",
    "!python generate.py \\\n",
    "    --output_dir tests/reverse-generation-de-en \\\n",
    "    --srclang de --tgtlang en \\\n",
    "    --load_from_checkpoint /home/mkrastev/nlp2/Models/it-mono+parallel/checkpoints/epoch=14-step=56250.ckpt \\\n",
    "    --batch_size 32 \\\n",
    "    --max_length 128 \\\n",
    "    --data_dir Data/it-mono --split train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we can fine-tune the model on the generated data\n",
    "!python train.py \\\n",
    "    --output_dir tests/reverse-generation-it-mono-de-en \\\n",
    "    --srclang de --tgtlang en \\\n",
    "    --batch_size 32 \\\n",
    "    --max_length 128 \\\n",
    "    --epochs 15 \\\n",
    "    --train_dir it-mono --seed 42 \\\n",
    "    --check_val_every_n_epoch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then generate translations for the IT-mono data and the forward model\n",
    "!python generate.py \\\n",
    "    --output_dir tests/iterative-generation-it-mono-en-de \\\n",
    "    --srclang en --tgtlang de \\\n",
    "    --load_from_checkpoint \n",
    "    --batch_size 32 \\\n",
    "    --max_length 128 \\\n",
    "    --data_dir it-mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally retrain fine-tune the model on the combined data\n",
    "!python train.py \\\n",
    "    --output_dir tests/iterative-generation-it-mono-en-de \\\n",
    "    --srclang en --tgtlang de \\\n",
    "    --batch_size 32 \\\n",
    "    --max_length 128 \\\n",
    "    --epochs 15 \\\n",
    "    --train_dir iterative-it-mono --seed 42 \\\n",
    "    --check_val_every_n_epoch 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And done! We have now successfully implemented iterative backtranslation. We can repeat this process until convergence (or until we run out of cloud compute, GPUs aren't cheap!)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
