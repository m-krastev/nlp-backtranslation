{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akDZZDOpkqtF"
      },
      "source": [
        "# The Backtranslation Notebook to Rule Over Them All\n",
        "Please do ignore the title, I just wanted to make it sound cool. This notebook is a simple demonstration of how to use backtranslation to improve the performance of a machine learning model. The notebook is divided into the following sections:\n",
        "1. Introduction and Simple Generation\n",
        "2. Applying Data Preparation/Filtering\n",
        "3. Investigating Iterative Backtranslation\n",
        "4. Applying a similar pipeline to a more complex model (ALMA-R)\n",
        "5. Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "AB8V1QkAl2h1"
      },
      "outputs": [],
      "source": [
        "# # Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Navigate to your project directory on Google Drive\n",
        "# import os\n",
        "# project_dir = '/content/drive/MyDrive/nlp-backtranslation'\n",
        "# os.chdir(project_dir)\n",
        "\n",
        "# # Install required packages\n",
        "# !pip install transformers\n",
        "# !pip install pytorch-lightning\n",
        "# !pip install peft\n",
        "# !pip install evaluate\n",
        "# !pip install sentencepiece\n",
        "# !pip install -U sacremoses\n",
        "# !pip install sacrebleu\n",
        "# !pip install unbabel-comet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na2S553hkqtH",
        "outputId": "cf9d7f7a-60ae-4b19-b629-f8ccd19c58d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# First cell in the notebook to enable autoreload of modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5Yf8iyQoZkA6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from calculate_bleu import calculate_bleu, calculate_comet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLc7lSyrkqtH",
        "outputId": "75452753-0234-4517-9d60-98c1be680a91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-de-en and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,179,648 || all params: 273,027,072 || trainable%: 0.4321\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
        "from utils.data import TranslationDataModule\n",
        "from utils.models import TranslationLightning\n",
        "from pytorch_lightning import Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "# Isn't it so nice and clean now? I went through FOUR different ways of doing this before I thought of this one. WDWFDGFEQWDQWFGA\n",
        "\n",
        "SRC = \"de\"\n",
        "TGT = \"en\"\n",
        "BATCH_SIZE = 16\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "cwd = Path.cwd()\n",
        "data_dir = cwd / \"Data\"\n",
        "\n",
        "it_parallel = \"it-parallel\"\n",
        "news_dataset = \"train-euro-news-big\"\n",
        "it_mono = \"it-mono\"\n",
        "generation_folder = \"generation+it-parallel-en-de\"\n",
        "\n",
        "test_folder = cwd / \"tests\"\n",
        "output_folder = test_folder / f\"generation-{TGT}-{SRC}\"\n",
        "\n",
        "mname = f\"facebook/wmt19-{SRC}-{TGT}\"\n",
        "tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
        "model = FSMTForConditionalGeneration.from_pretrained(mname)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r = 16,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.2,\n",
        "    target_modules = [\"v_proj\", \"q_proj\"]\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath='/content/drive/MyDrive/checkpoints',\n",
        "    save_top_k=1,\n",
        "    monitor='val_loss',\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# it_parallel_data = TranslationDataModule(data_dir / it_parallel, SRC, TGT, tokenizer, None, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
        "# # Add news_dataset\n",
        "# combined_data_module = TranslationDataModule(data_dir,\n",
        "#                                              SRC,\n",
        "#                                              TGT,\n",
        "#                                              tokenizer,\n",
        "#                                              None,\n",
        "#                                              batch_size=BATCH_SIZE,\n",
        "#                                              max_length=MAX_LENGTH,\n",
        "#                                              use_combined_data=True,\n",
        "#                                              generation_folder=generation_folder)\n",
        "# combined_data_module.setup('fit')\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "#model_pl = TranslationLightning(model, tokenizer, lr=3e-4, adam_beta=(0.9, 0.98), weight_decay=1e-4, output_dir = output_folder)\n",
        "\n",
        "trainer = Trainer(max_epochs=5, check_val_every_n_epoch=5, gradient_clip_val=0.3, val_check_interval = 0.25, limit_val_batches=0.25, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Applying data preparation /filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6Vf5l7KUZiH",
        "outputId": "9631509b-fe9a-43a6-c548-8959324177ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-de-en and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,179,648 || all params: 273,027,072 || trainable%: 0.4321\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "from evaluate import load\n",
        "from statistics import stdev\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
        "from pytorch_lightning import Trainer\n",
        "from utils.data import TranslationDataModule\n",
        "from utils.models import TranslationLightning\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Define constants\n",
        "SRC = \"de\"\n",
        "TGT = \"en\"\n",
        "BATCH_SIZE = 16\n",
        "MAX_LENGTH = 128\n",
        "cwd = Path.cwd()\n",
        "data_dir = cwd / \"Data\"\n",
        "generation_folder = \"generation+it-parallel-en-de\"\n",
        "output_folder_base = cwd / \"results\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "mname = f\"facebook/wmt19-{SRC}-{TGT}\"\n",
        "tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
        "model = FSMTForConditionalGeneration.from_pretrained(mname)\n",
        "config = LoraConfig(r=16, lora_alpha=16, lora_dropout=0.2, target_modules=[\"v_proj\", \"q_proj\"])\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "def calculate_bleu(hyps: list[str], refs: list[str]):\n",
        "    sacrebleu = load(\"sacrebleu\")\n",
        "    bleu = sacrebleu.compute(predictions=hyps, references=refs)\n",
        "    return bleu\n",
        "\n",
        "def calculate_comet(hyps: list[str], refs: list[str], src: list[str]):\n",
        "    comet = load(\"comet\")\n",
        "    score = comet.compute(predictions=hyps, references=refs, sources=src)\n",
        "    score = f\"COMET score: {score['mean_score']:.3f}±{stdev(score['scores']):.3f}\"\n",
        "    return score\n",
        "\n",
        "def train_and_evaluate(top_percentage=None, log_file='training_log.csv'):\n",
        "    output_folder = output_folder_base / f\"{top_percentage}_percent\"\n",
        "    output_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    combined_data_module = TranslationDataModule(\n",
        "        data_dir,\n",
        "        SRC,\n",
        "        TGT,\n",
        "        tokenizer,\n",
        "        None,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        max_length=MAX_LENGTH,\n",
        "        use_combined_data=(top_percentage is not None),\n",
        "        generation_folder=generation_folder,\n",
        "        top_percentage=top_percentage if top_percentage else 1.0\n",
        "    )\n",
        "    combined_data_module.setup('fit')\n",
        "\n",
        "    model_pl = TranslationLightning(model, tokenizer, lr=3e-4, adam_beta=(0.9, 0.98), weight_decay=1e-4, output_dir=output_folder)\n",
        "\n",
        "    # Check the first few samples of the train dataset\n",
        "    for i in range(5):\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(\"Source:\", combined_data_module.train.src_texts[i])\n",
        "        print(\"Target:\", combined_data_module.train.tgt_texts[i])\n",
        "\n",
        "    # Check the dataset sizes\n",
        "    print(\"Train dataset size:\", len(combined_data_module.train))\n",
        "    print(\"Validation dataset size:\", len(combined_data_module.val))\n",
        "    print(\"Test dataset size:\", len(combined_data_module.test))\n",
        "\n",
        "    # Setup the trainer\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=output_folder,\n",
        "        filename=\"{epoch}-{val_loss:.2f}\",\n",
        "        monitor=\"val_loss\",\n",
        "        save_top_k=1,\n",
        "        mode=\"min\",\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        max_epochs=5,\n",
        "        check_val_every_n_epoch=1,\n",
        "        gradient_clip_val=0.3,\n",
        "        val_check_interval=0.25,\n",
        "        limit_val_batches=0.25,\n",
        "        callbacks=[checkpoint_callback]\n",
        "    )\n",
        "\n",
        "    # Train and evaluate\n",
        "    trainer.fit(model_pl, datamodule=combined_data_module)\n",
        "    results = trainer.predict(model_pl, datamodule=combined_data_module)\n",
        "    bleu = sum(results) / len(results)\n",
        "    print(\"Average BLEU:\", bleu)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    with open(output_folder / \"hypothesis.hyp\", \"r\") as hyp_file:\n",
        "        hyps = hyp_file.readlines()\n",
        "    with open(output_folder / \"reference.ref\", \"r\") as ref_file:\n",
        "        refs = ref_file.readlines()\n",
        "    bleu_score = calculate_bleu(hyps, refs)\n",
        "    print(\"BLEU score:\", bleu_score)\n",
        "\n",
        "    # Calculate COMET score if source file exists\n",
        "    src_file = output_folder / \"source.src\"\n",
        "    if src_file.exists():\n",
        "        with open(src_file, \"r\") as src_file:\n",
        "            src = src_file.readlines()\n",
        "        comet_score = calculate_comet(hyps, refs, src)\n",
        "        print(\"COMET score:\", comet_score)\n",
        "\n",
        "    # Log results to CSV\n",
        "    log_data = {\n",
        "        'top_percentage': top_percentage,\n",
        "        'train_loss': model_pl.trainer.logged_metrics.get('train_loss', None),\n",
        "        'val_loss': model_pl.trainer.logged_metrics.get('val_loss', None),\n",
        "        'val_bleu': model_pl.trainer.logged_metrics.get('val_bleu', None),\n",
        "        'average_bleu': bleu,\n",
        "        'bleu_score': bleu_score['score']\n",
        "    }\n",
        "    if 'comet_score' in locals():\n",
        "        log_data['comet_score'] = comet_score\n",
        "\n",
        "    df = pd.DataFrame([log_data])\n",
        "    if not Path(log_file).exists():\n",
        "        df.to_csv(log_file, index=False)\n",
        "    else:\n",
        "        df.to_csv(log_file, mode='a', header=False, index=False)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training with different diversity metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "toHCa58zxFMZ"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate with 50% data\n",
        "print(\"Training with 50% data:\")\n",
        "train_and_evaluate(top_percentage=0.5, log_file='training_log.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cERWiAa56gLw"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate with 30% data\n",
        "print(\"Training with 30% data:\")\n",
        "train_and_evaluate(top_percentage=0.3, log_file='training_log.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "c53wn2DzxQwp"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate with 70% data\n",
        "print(\"Training with 70% data:\")\n",
        "train_and_evaluate(top_percentage=0.7, log_file='training_log.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "l5hXig876esL"
      },
      "outputs": [],
      "source": [
        "# Ablation study: Train and evaluate without using the metric (use all data)\n",
        "print(\"Ablation study: Using all data without applying the metric:\")\n",
        "train_and_evaluate(top_percentage=None, log_file='training_log.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5u_2dDvkqtJ"
      },
      "outputs": [],
      "source": [
        "it_parallel_data = TranslationDataModule(data_dir / it_parallel, SRC, TGT, tokenizer, model, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
        "\n",
        "it_parallel_data.setup('fit')\n",
        "dataset = it_parallel_data.train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY1cUc_vkqtJ",
        "outputId": "5cbe419b-e5f5-4498-f3ca-5730f5705c4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(it_parallel_data.test_dataloader()))\n",
        "# batch['labels'][batch['labels']==1] = -100\n",
        "# batch.pop('decoder_input_ids')\n",
        "print(batch.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQVrzkwKkqtJ",
        "outputId": "f19f7245-4d7a-4592-ec8a-649d645a65fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.9212, grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_pl.training_step(batch, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nS4torIkqtJ"
      },
      "outputs": [],
      "source": [
        "it_parallel_data.setup('fit')\n",
        "# batch = next(iter(it_parallel_data.train_dataloader()))\n",
        "\n",
        "# print(batch[\"labels\"].)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKGrb3lekqtK"
      },
      "source": [
        "## Simple Generation\n",
        "Here we simply load a pre-trained model and run inference on the test set. The model loaded is a pre-trained model from the [Hugging Face Transformers](https://huggingface.co/transformers/) library.\n",
        "\n",
        "TODO: Evaluate the model on the NEWS set and report the BLEU score.\n",
        "\n",
        "| Model | it-parallel | NEWS |\n",
        "| --- | --- | --- |\n",
        "| Base | 38.307622648987454 (0.819±.145) | 29.226±7.710 (0.834±0.077) |\n",
        "| FT on IT-parallel | 38.919114415962934 (0.825±.145) | 28.646 (0.827±0.064) |\n",
        "| FT on BT | 41.827 (0.84±.0) | 27.115±7.197 (0.820±0.084) |\n",
        "| FT on IT-parallel and BT | 39.879 (0.86±.1) | 26.655±7.249 (0.819±0.084) |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpq-dqGvkqtK"
      },
      "outputs": [],
      "source": [
        "# IT-parallel\n",
        "results = trainer.predict(model_pl, datamodule=it_parallel_data)\n",
        "bleu = sum(r[2][\"score\"] for r in results) / len(results)\n",
        "print(\"Average BLEU:\", bleu)\n",
        "\n",
        "# # News corpus\n",
        "# # ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6X4h8VwkqtK"
      },
      "outputs": [],
      "source": [
        "# Now we can train the model\n",
        "trainer.fit(model_pl, datamodule=it_parallel_data)\n",
        "\n",
        "# IT-parallel\n",
        "trainer.predict(model_pl, datamodule=it_parallel_data)\n",
        "bleu = sum(r[2][\"score\"] for r in results) / len(results)\n",
        "print(\"Average BLEU:\", bleu)\n",
        "\n",
        "# # News corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SIaAvWCkqtK"
      },
      "source": [
        "### Now the key ingredient: Backtranslation\n",
        "Backtranslation uses a reverse model to generate synthetic data. This synthetic data is then used to train the model. The idea is that the synthetic data will help the model generalize better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUSZp-APkqtK"
      },
      "outputs": [],
      "source": [
        "reverse_model = FSMTForConditionalGeneration.from_pretrained(f\"facebook/wmt19-{TGT}-{SRC}\")\n",
        "reverse_tokenizer = FSMTTokenizer.from_pretrained(f\"facebook/wmt19-{TGT}-{SRC}\")\n",
        "\n",
        "it_mono = TranslationDataModule(data_dir / it_mono, TGT, SRC, reverse_tokenizer, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
        "\n",
        "output_folder = test_folder / f\"generation-{TGT}-{SRC}\"\n",
        "reverse_lightning = TranslationLightning(reverse_model, reverse_tokenizer, lr=3e-4, adam_beta=(0.9, 0.98), weight_decay=1e-4, test_folder = output_folder)\n",
        "\n",
        "trainer.predict(reverse_lightning, it_mono.train_dataloader())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R17FiZ_zkqtK"
      },
      "source": [
        "Now that we have generated the data, we can now copy in the generated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B73hTy_7kqtK"
      },
      "outputs": [],
      "source": [
        "train_file = output_folder / \"hypothesis.hyp\"\n",
        "train_file_target = output_folder / \"source.src\"\n",
        "train_file.rename(data_dir / f\"generated-{it_mono}\" / f\"train.{SRC}\")\n",
        "train_file_target.rename(data_dir / f\"generated-{it_mono}\" / f\"train.{TGT}\")\n",
        "\n",
        "import shutil\n",
        "bt_dir = data_dir / f\"generated-{it_mono}\"\n",
        "shutil.copy(data_dir / it_mono / f\"dev.{SRC}\", bt_dir / f\"dev.{SRC}\")\n",
        "shutil.copy(data_dir / it_mono / f\"dev.{TGT}\", bt_dir / f\"dev.{TGT}\")\n",
        "shutil.copy(data_dir / it_mono / f\"test.{SRC}\", bt_dir / f\"test.{SRC}\")\n",
        "shutil.copy(data_dir / it_mono / f\"test.{TGT}\", bt_dir / f\"test.{TGT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8jAk6n6kqtL"
      },
      "outputs": [],
      "source": [
        "# And now we can finally train...\n",
        "it_mono_bt = TranslationDataModule(data_dir / f\"generated-{it_mono}\", SRC, TGT, tokenizer, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
        "trainer.fit(model_pl, datamodule=it_mono_bt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9MvZzE0kqtL"
      },
      "outputs": [],
      "source": [
        "model_pl.test_folder = test_folder / f\"bt_{it_mono}\"\n",
        "# IT-parallel\n",
        "results = trainer.predict(model_pl, datamodule=it_parallel_data)\n",
        "print(\"IT-parallel\", sum(r[2][\"score\"] for r in results) / len(results))\n",
        "# News corpus\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0MkuywFkqtL"
      },
      "source": [
        "And now finally we can concatenate both the original and the generated data and train the model on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_1M4X2GkqtL"
      },
      "outputs": [],
      "source": [
        "parallel_plus_bt = data_dir / f\"parallel+bt_generated_{it_mono}\"\n",
        "shutil.copytree(bt_dir, parallel_plus_bt)\n",
        "train_file = parallel_plus_bt /f\"train.{SRC}\"\n",
        "with open(train_file, \"r+\") as f:\n",
        "    with open(data_dir / it_parallel / f\"train.{SRC}\", \"r\") as f2:\n",
        "        f.write(\"\\n\" + f2.read())\n",
        "\n",
        "train_file_target = parallel_plus_bt / f\"train.{TGT}\"\n",
        "with open(train_file_target, \"r+\") as f:\n",
        "    with open(data_dir / it_parallel / f\"train.{TGT}\", \"r\") as f2:\n",
        "        f.write(\"\\n\" + f2.read())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tVJh2NDkqtL"
      },
      "outputs": [],
      "source": [
        "# Fitting the model on the combined data\n",
        "parallel_plus_bt_data = TranslationDataModule(parallel_plus_bt, SRC, TGT, tokenizer, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
        "trainer.fit(model_pl, datamodule=parallel_plus_bt_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF1PvcffkqtL"
      },
      "outputs": [],
      "source": [
        "# IT-parallel\n",
        "results = trainer.predict(model_pl, datamodule=it_parallel_data)\n",
        "print(\"IT-parallel BLEU:\", sum(r[2][\"score\"] for r in results) / len(results))\n",
        "# News corpus\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA1I3BltkqtL"
      },
      "source": [
        "## 3. Investigating Iterative Backtranslation\n",
        "The basic idea, proposed by Hoang et al. (2018), is to iteratively backtranslate the data. For our purposes, we require two separate models, one for forward translation (EN-DE) and one for reverse translation (DE-EN). We can then use them in turns, first generating synthetic data, training the model, and then using the trained forward model to generate synthetic data for the reverse model. This process may be iterated until convergence.\n",
        "\n",
        "We can combine this approach with concatenating the high quality parallel data to improve the performance of both models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-gj3C7kkqtL"
      },
      "outputs": [],
      "source": [
        "# First we might want to fine-tune the model on the IT-parallel data\n",
        "!python train.py \\\n",
        "    --output_dir tests/reverse-it-parallel-de-en \\\n",
        "    --srclang de --tgtlang en \\\n",
        "    --batch_size 32 \\\n",
        "    --max_length 128 \\\n",
        "    --epochs 15 \\\n",
        "    --train_dir it-parallel --seed 42 \\\n",
        "    --check_val_every_n_epoch 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYfVdFcKkqtL"
      },
      "outputs": [],
      "source": [
        "# Now we can generate translations for the IT-mono data\n",
        "!python generate.py \\\n",
        "    --output_dir tests/reverse-generation-de-en \\\n",
        "    --srclang de --tgtlang en \\\n",
        "    --load_from_checkpoint /home/mkrastev/nlp2/Models/it-mono+parallel/checkpoints/epoch=14-step=56250.ckpt \\\n",
        "    --batch_size 32 \\\n",
        "    --max_length 128 \\\n",
        "    --data_dir Data/it-mono --split train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LguOC9iPkqtL"
      },
      "outputs": [],
      "source": [
        "# And now we can fine-tune the model on the generated data\n",
        "!python train.py \\\n",
        "    --output_dir tests/reverse-generation-it-mono-de-en \\\n",
        "    --srclang de --tgtlang en \\\n",
        "    --batch_size 32 \\\n",
        "    --max_length 128 \\\n",
        "    --epochs 15 \\\n",
        "    --train_dir it-mono --seed 42 \\\n",
        "    --check_val_every_n_epoch 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP0w709mkqtM"
      },
      "outputs": [],
      "source": [
        "# Then generate translations for the IT-mono data and the forward model\n",
        "!python generate.py \\\n",
        "    --output_dir tests/iterative-generation-it-mono-en-de \\\n",
        "    --srclang en --tgtlang de \\\n",
        "    --load_from_checkpoint\n",
        "    --batch_size 32 \\\n",
        "    --max_length 128 \\\n",
        "    --data_dir it-mono"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AF__uH8kqtM"
      },
      "outputs": [],
      "source": [
        "# And finally retrain fine-tune the model on the combined data\n",
        "!python train.py \\\n",
        "    --output_dir tests/iterative-generation-it-mono-en-de \\\n",
        "    --srclang en --tgtlang de \\\n",
        "    --batch_size 32 \\\n",
        "    --max_length 128 \\\n",
        "    --epochs 15 \\\n",
        "    --train_dir iterative-it-mono --seed 42 \\\n",
        "    --check_val_every_n_epoch 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5dGAFHykqtM"
      },
      "source": [
        "And done! We have now successfully implemented iterative backtranslation. We can repeat this process until convergence (or until we run out of cloud compute, GPUs aren't cheap!)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
