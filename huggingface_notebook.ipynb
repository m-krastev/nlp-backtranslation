{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Backtranslation Notebook to Rule Over Them All\n",
    "Please do ignore the title, I just wanted to make it sound cool. This notebook is a simple demonstration of how to use backtranslation to improve the performance of a machine learning model. The notebook is divided into the following sections:\n",
    "1. Introduction and Simple Generation\n",
    "2. Applying Data Preparation/Filtering\n",
    "3. Investigating Iterative Backtranslation\n",
    "4. Applying a similar pipeline to a more complex model (ALMA-R)\n",
    "5. Conclusion\n",
    "\n",
    "Results:\n",
    "\n",
    "| Model | IT |  | NEWS | |\n",
    "| ---   |---- | --- | --- | --- |\n",
    "|       | BLEU| COMET|BLEU|COMET|\n",
    "| Base | 38.31 (0.819±.145) | 29.226±7.710 (0.834±0.077) |\n",
    "| FT on IT-parallel | 38.92 (0.825±.145) | 28.646 (0.827±0.064) |\n",
    "| FT on BT | 41.827 (0.84±.0) | 27.115±7.197 (0.820±0.084) |\n",
    "| FT on IT-parallel and BT | 39.879 (0.86±.1) | 26.655±7.249 (0.819±0.084) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cell in the notebook to enable autoreload of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matey/project/nlp2/.venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-de-en and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
    "from utils.data import TranslationDataModule\n",
    "from utils.models import TranslationLightning\n",
    "from pytorch_lightning import Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "# Isn't it so nice and clean now? I went through FOUR different ways of doing this before I thought of this one. WDWFDGFEQWDQWFGA\n",
    "\n",
    "SRC = \"de\"\n",
    "TGT = \"en\"\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "cwd = Path.cwd()\n",
    "data_dir = cwd / \"Data\"\n",
    "\n",
    "it_parallel = \"it-parallel\"\n",
    "news_dataset = \"train-euro-news-big\"\n",
    "it_mono = \"it-mono\"\n",
    "\n",
    "test_folder = cwd / \"tests\"\n",
    "\n",
    "mname = f\"facebook/wmt19-{SRC}-{TGT}\"\n",
    "tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
    "# model = FSMTForConditionalGeneration.from_pretrained(mname)\n",
    "\n",
    "# config = LoraConfig(\n",
    "#     r = 16,\n",
    "#     lora_alpha = 16,\n",
    "#     lora_dropout = 0.2,\n",
    "#     target_modules = [\"v_proj\", \"q_proj\"]\n",
    "# )\n",
    "\n",
    "# it_parallel_data = TranslationDataModule(data_dir / it_parallel, SRC, TGT, tokenizer, None, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "# # Add news_dataset\n",
    "\n",
    "\n",
    "# model = get_peft_model(model, config)\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "# model_pl = TranslationLightning(model, tokenizer, lr=3e-4, adam_beta=(0.9, 0.98), weight_decay=1e-4, test_folder = test_folder)\n",
    "\n",
    "# trainer = Trainer(max_steps=10,  gradient_clip_val=0.1, val_check_interval = 0.25, limit_val_batches=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_parallel_data = TranslationDataModule(data_dir / it_parallel, SRC, TGT, tokenizer, model, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "\n",
    "it_parallel_data.setup('fit')\n",
    "dataset = it_parallel_data.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(it_parallel_data.test_dataloader()))\n",
    "# batch['labels'][batch['labels']==1] = -100\n",
    "# batch.pop('decoder_input_ids')\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9212, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pl.training_step(batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_parallel_data.setup('fit')\n",
    "# batch = next(iter(it_parallel_data.train_dataloader()))\n",
    "\n",
    "# print(batch[\"labels\"].)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Generation\n",
    "Here we simply load a pre-trained model and run inference on the test set. The model loaded is a pre-trained model from the [Hugging Face Transformers](https://huggingface.co/transformers/) library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT-parallel\n",
    "results = trainer.predict(model_pl, datamodule=it_parallel_data)\n",
    "bleu = sum(r[2][\"score\"] for r in results) / len(results)\n",
    "print(\"Average BLEU:\", bleu)\n",
    "\n",
    "# News corpus\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can train the model\n",
    "trainer.fit(model_pl, datamodule=it_parallel_data)\n",
    "\n",
    "# IT-parallel\n",
    "trainer.predict(model_pl, datamodule=it_parallel_data)\n",
    "bleu = sum(r[2][\"score\"] for r in results) / len(results)\n",
    "print(\"Average BLEU:\", bleu)\n",
    "\n",
    "# News corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the key ingredient: Backtranslation\n",
    "Backtranslation uses a reverse model to generate synthetic data. This synthetic data is then used to train the model. The idea is that the synthetic data will help the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_model = FSMTForConditionalGeneration.from_pretrained(f\"facebook/wmt19-{TGT}-{SRC}\")\n",
    "reverse_tokenizer = FSMTTokenizer.from_pretrained(f\"facebook/wmt19-{TGT}-{SRC}\")\n",
    "\n",
    "it_mono = TranslationDataModule(data_dir / it_mono, TGT, SRC, reverse_tokenizer, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "\n",
    "output_folder = test_folder / f\"generation-{TGT}-{SRC}\"\n",
    "reverse_lightning = TranslationLightning(reverse_model, reverse_tokenizer, lr=3e-4, adam_beta=(0.9, 0.98), weight_decay=1e-4, test_folder = output_folder)\n",
    "\n",
    "trainer.predict(reverse_lightning, it_mono.train_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated the data, we can now copy in the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = output_folder / \"hypothesis.hyp\"\n",
    "train_file_target = output_folder / \"source.src\"\n",
    "train_file.rename(data_dir / f\"generated-{it_mono}\" / f\"train.{SRC}\")\n",
    "train_file_target.rename(data_dir / f\"generated-{it_mono}\" / f\"train.{TGT}\")\n",
    "\n",
    "import shutil\n",
    "bt_dir = data_dir / f\"generated-{it_mono}\"\n",
    "shutil.copy(data_dir / it_mono / f\"dev.{SRC}\", bt_dir / f\"dev.{SRC}\")\n",
    "shutil.copy(data_dir / it_mono / f\"dev.{TGT}\", bt_dir / f\"dev.{TGT}\")\n",
    "shutil.copy(data_dir / it_mono / f\"test.{SRC}\", bt_dir / f\"test.{SRC}\")\n",
    "shutil.copy(data_dir / it_mono / f\"test.{TGT}\", bt_dir / f\"test.{TGT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we can finally train...\n",
    "it_mono_bt = TranslationDataModule(data_dir / f\"generated-{it_mono}\", SRC, TGT, tokenizer, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "trainer.fit(model_pl, datamodule=it_mono_bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pl.test_folder = test_folder / f\"bt_{it_mono}\"\n",
    "# IT-parallel\n",
    "results = trainer.predict(model_pl, datamodule=it_parallel_data)\n",
    "print(\"IT-parallel\", sum(r[2][\"score\"] for r in results) / len(results))\n",
    "# News corpus\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now finally we can concatenate both the original and the generated data and train the model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_plus_bt = data_dir / f\"parallel+bt_generated_{it_mono}\"\n",
    "shutil.copytree(bt_dir, parallel_plus_bt)\n",
    "train_file = parallel_plus_bt /f\"train.{SRC}\"\n",
    "with open(train_file, \"r+\") as f:\n",
    "    with open(data_dir / it_parallel / f\"train.{SRC}\", \"r\") as f2:\n",
    "        f.write(\"\\n\" + f2.read())\n",
    "        \n",
    "train_file_target = parallel_plus_bt / f\"train.{TGT}\"\n",
    "with open(train_file_target, \"r+\") as f:\n",
    "    with open(data_dir / it_parallel / f\"train.{TGT}\", \"r\") as f2:\n",
    "        f.write(\"\\n\" + f2.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model on the combined data\n",
    "parallel_plus_bt_data = TranslationDataModule(parallel_plus_bt, SRC, TGT, tokenizer, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
    "trainer.fit(model_pl, datamodule=parallel_plus_bt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IT-parallel\n",
    "results = trainer.predict(model_pl, datamodule=it_parallel_data)\n",
    "print(\"IT-parallel BLEU:\", sum(r[2][\"score\"] for r in results) / len(results))\n",
    "# News corpus\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Investigating Iterative Backtranslation\n",
    "The basic idea, proposed by Hoang et al. (2018), is to iteratively backtranslate the data. For our purposes, we require two separate models, one for forward translation (EN-DE) and one for reverse translation (DE-EN). We can then use them in turns, first generating synthetic data, training the model, and then using the trained forward model to generate synthetic data for the reverse model. This process may be iterated until convergence.\n",
    "\n",
    "We can combine this approach with concatenating the high quality parallel data to improve the performance of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we might want to fine-tune the model on the IT-parallel data\n",
    "!python train.py \\\n",
    "    --output_dir tests/reverse-it-parallel-de-en \\\n",
    "    --srclang de --tgtlang en \\\n",
    "    --batch_size 32 \\\n",
    "    --max_length 128 \\\n",
    "    --epochs 15 \\\n",
    "    --train_dir it-parallel --seed 42 \\\n",
    "    --check_val_every_n_epoch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can generate translations for the IT-mono data\n",
    "!python generate.py \\\n",
    "    --output_dir tests/reverse-generation-de-en \\\n",
    "    --srclang de --tgtlang en \\\n",
    "    --load_from_checkpoint /home/mkrastev/nlp2/Models/it-mono+parallel/checkpoints/epoch=14-step=56250.ckpt \\\n",
    "    --batch_size 32 \\\n",
    "    --max_length 128 \\\n",
    "    --data_dir Data/it-mono --split train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we can fine-tune the model on the generated data\n",
    "!python train.py \\\n",
    "    --output_dir tests/reverse-generation-it-mono-de-en \\\n",
    "    --srclang de --tgtlang en \\\n",
    "    --batch_size 32 \\\n",
    "    --max_length 128 \\\n",
    "    --epochs 15 \\\n",
    "    --train_dir it-mono --seed 42 \\\n",
    "    --check_val_every_n_epoch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then generate translations for the IT-mono data and the forward model\n",
    "!python generate.py \\\n",
    "    --output_dir tests/iterative-generation-it-mono-en-de \\\n",
    "    --srclang en --tgtlang de \\\n",
    "    --load_from_checkpoint \n",
    "    --batch_size 32 \\\n",
    "    --max_length 128 \\\n",
    "    --data_dir it-mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally retrain fine-tune the model on the combined data\n",
    "!python train.py \\\n",
    "    --output_dir tests/iterative-generation-it-mono-en-de \\\n",
    "    --srclang en --tgtlang de \\\n",
    "    --batch_size 32 \\\n",
    "    --max_length 128 \\\n",
    "    --epochs 15 \\\n",
    "    --train_dir iterative-it-mono --seed 42 \\\n",
    "    --check_val_every_n_epoch 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And done! We have now successfully implemented iterative backtranslation. We can repeat this process until convergence (or until we run out of cloud compute, GPUs aren't cheap!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher-student Training\n",
    "Would be an overstatement to use the term teacher-student training here, as it is more applicable to different types of neural networks, and in particular our focus was Direct Preference Optimization (DPO) learning, however, that is usually reserved for proper text generation language models.\n",
    "\n",
    "Instead, here we make use of a more intelligent system in order to generate synthetic data. We can use a more complex model to generate synthetic data, and then use that data to train a simpler model. This is a form of transfer learning, where the more complex model is used to transfer knowledge to the simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DEEPL_AUTH_KEY=cf6338ba-8acc-c8a9-95ef-1aa28fb41033:fx\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Translation failed with response 456 and data: {\"message\":\"Quota Exceeded\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m stack\u001b[38;5;241m.\u001b[39mappend(line\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m     translated \u001b[38;5;241m=\u001b[39m \u001b[43mdeepl_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeepl_auth_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     45\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m translated[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslations\u001b[39m\u001b[38;5;124m\"\u001b[39m]]))\n",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m, in \u001b[0;36mdeepl_translate\u001b[0;34m(text, deepl_auth_key)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# print(response)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslation failed with response \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mException\u001b[0m: Translation failed with response 456 and data: {\"message\":\"Quota Exceeded\"}"
     ]
    }
   ],
   "source": [
    "%env DEEPL_AUTH_KEY=cf6338ba-8acc-c8a9-95ef-1aa28fb41033:fx\n",
    "\n",
    "import os\n",
    "\n",
    "from requests import post\n",
    "\n",
    "out_dir = data_dir / \"deepl-it-mono\"\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "input_file = data_dir / \"it-mono\" / f\"train.{TGT}\"\n",
    "output_file = out_dir / f\"train.{SRC}\"\n",
    "\n",
    "deepl_auth_key = None\n",
    "if deepl_auth_key is None:\n",
    "    deepl_auth_key = os.getenv(\"DEEPL_AUTH_KEY\")\n",
    "\n",
    "def deepl_translate(text: list[str], deepl_auth_key):\n",
    "    body = {\n",
    "        \"text\": text,\n",
    "        \"source_lang\": TGT,\n",
    "        \"target_lang\": SRC,\n",
    "        \"preserve_formatting\": True,\n",
    "        \"tag_handling\": \"xml\",\n",
    "        \"outline_detection\": False,\n",
    "        \"split_sentences\": \"nonewlines\",\n",
    "    }\n",
    "    response = post(\n",
    "        \"https://api-free.deepl.com/v2/translate\",\n",
    "        json=body,\n",
    "        headers={\n",
    "            \"Authorization\": f\"DeepL-Auth-Key {deepl_auth_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        },\n",
    "    )\n",
    "    # print(response)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Translation failed with response {response.status_code} and data: {response.text}\"\n",
    "        )\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "with open(input_file, encoding=\"utf8\") as f:\n",
    "    stack  = [ ]\n",
    "    for i, line in enumerate(f):\n",
    "        stack.append(line.strip())\n",
    "        if i % 1000 == 0:\n",
    "            translated = deepl_translate(stack, deepl_auth_key)\n",
    "            with open(output_file, \"a\", encoding=\"utf8\") as f:\n",
    "                f.write(\"\\n\".join([t[\"text\"] for t in translated[\"translations\"]])+\"\\n\")\n",
    "            stack = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
