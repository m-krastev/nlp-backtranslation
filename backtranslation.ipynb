{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Backtranslation\n",
    "Let's define some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "from typing import Dict, List, Union, Tuple\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "SRC = \"de\"\n",
    "TGT = \"en\"\n",
    "\n",
    "\n",
    "cwd = Path.cwd()\n",
    "data_dir = cwd / \"Data\"\n",
    "model_dir = cwd / \"Models\" / \"hugging_face\"\n",
    "\n",
    "it_parallel = \"it-parallel\"\n",
    "news_dataset = \"train-euro-news-big\"\n",
    "it_mono = \"it-mono\"\n",
    "\n",
    "test_folder = cwd / \"tests\"\n",
    "\n",
    "sentencepiece_script = cwd / \"spm_encode.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, how are you?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "base_model_en_de = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"checkpoint_best-en-de.pt\", local_dir=model_dir)\n",
    "base_model_en_de = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"checkpoint_best-de-en.pt\", local_dir=model_dir)\n",
    "dict_de = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"dict.de.txt\", local_dir=model_dir)\n",
    "dict_en = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"dict.en.txt\", local_dir=model_dir)\n",
    "sentencepiece_model = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"spm.model\", local_dir=model_dir)\n",
    "\n",
    "from tokenizers import SentencePieceUnigramTokenizer\n",
    "tokenizer = SentencePieceUnigramTokenizer.from_spm(\n",
    "    sentencepiece_model\n",
    ")\n",
    "\n",
    "tokens = tokenizer.encode(\"Hello, how are you?\")\n",
    "tokenizer.decode(tokens.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(args: List[str]):\n",
    "    with subprocess.Popen(\n",
    "        args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, shell=True\n",
    "    ) as proc:\n",
    "        for line in proc.stdout:\n",
    "            print(line)\n",
    "    return proc\n",
    "\n",
    "def print_file(path: Path):\n",
    "    with path.open() as f:\n",
    "        print(f.read())\n",
    "\n",
    "def get_train_model_args(\n",
    "    path_to_data,\n",
    "    arch=\"transformer_wmt_en_de\",\n",
    "    max_update=10,\n",
    "    model_dir: Path | str =\"Models\",\n",
    "    experiment_name=\"test-de-en\",\n",
    "    lr=6e-4,\n",
    "):\n",
    "    return [\n",
    "        \"fairseq-train\",\n",
    "        str(path_to_data),\n",
    "        \"--arch\",\n",
    "        arch,\n",
    "        \"--task translation\",\n",
    "        \"--share-decoder-input-output-embed\",\n",
    "        \"--optimizer adam\",\n",
    "        \"--adam-betas '(0.9, 0.98)'\",\n",
    "        \"--clip-norm 0.1\",\n",
    "        \"--lr\",\n",
    "        lr,\n",
    "        \"--lr-scheduler inverse_sqrt\",\n",
    "        \"--warmup-updates 2500\",\n",
    "        \"--warmup-init-lr 1e-07\",\n",
    "        \"--stop-min-lr 1e-09\",\n",
    "        \"--dropout 0.3\",\n",
    "        \"--weight-decay 0.0001\",\n",
    "        \"--criterion label_smoothed_cross_entropy\",\n",
    "        \"--label-smoothing 0.1\",\n",
    "        \"--max-tokens 8192\",\n",
    "        \"--max-update\",\n",
    "        max_update,\n",
    "        \"--update-freq 8\",\n",
    "        \"--patience 10\",\n",
    "        \"--scoring sacrebleu\",\n",
    "        \"--eval-bleu\",\n",
    "        '--eval-bleu-args \\'{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}\\'',\n",
    "        \"--eval-bleu-detok moses\",\n",
    "        \"--eval-bleu-remove-bpe\",\n",
    "        \"--eval-bleu-print-samples\",\n",
    "        \"--best-checkpoint-metric bleu\",\n",
    "        \"--maximize-best-checkpoint-metric\",\n",
    "        \"--save-interval-updates 2000\",\n",
    "        \"--validate-interval-updates 2000\",\n",
    "        \"--keep-best-checkpoints 1\",\n",
    "        \"--encoder-learned-pos\",\n",
    "        \"--save-dir\",\n",
    "        str(model_dir) + \"/\" + experiment_name,\n",
    "        \"--bpe sentencepiece\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_bleu_score(experiment: Path):\n",
    "    \"\"\"Returns the command to calculate the BLEU score. Final path component is the result file.\"\"\"\n",
    "    # cat $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.hyp | sacrebleu $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.ref -m bleu > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.sacrebleu\n",
    "    return \"cat {0}.hyp | sacrebleu {0}.ref -m bleu > {0}.sacrebleu\".format(\n",
    "        experiment\n",
    "    ), experiment.with_suffix(\".sacrebleu\")\n",
    "\n",
    "\n",
    "# [\n",
    "\n",
    "#         \"cat\",\n",
    "#         str(experiment.with_suffix(\".hyp\")),\n",
    "#         \"| sacrebleu\",\n",
    "#         str(experiment.with_suffix(\".ref\")),\n",
    "#         \"-m\",\n",
    "#         \"bleu >\",\n",
    "#        str( experiment.with_suffix(\".sacrebleu\")),\n",
    "#     ]\n",
    "\n",
    "\n",
    "def generate_args(\n",
    "    path_to_data: str | Path,\n",
    "    subset: str,\n",
    "    src: str,\n",
    "    tgt: str,\n",
    "    model_checkpoint: str | Path,\n",
    "    save_dir: str,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates the arguments for the fairseq-generate command\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_data : str\n",
    "        Path to the data directory (the data must be binarized)\n",
    "    subset : str\n",
    "        The subset to generate the outputs for (e.g., test)\n",
    "    src : str\n",
    "        The source language\n",
    "    tgt : str\n",
    "        The target language\n",
    "    model_checkpoint : str\n",
    "        The path to the model checkpoint\n",
    "    \"\"\"\n",
    "    # fairseq-generate Data/$TEST/bin \\\n",
    "    #  --gen-subset test --source-lang $SRC --target-lang $TGT \\\n",
    "    #  --path./Models/$MODEL/checkpoint_best.pt \\\n",
    "    #  --skip-invalid-size-inputs-valid-test \\\n",
    "    #  --batch-size 128 --beam 5 --remove-bpe sentencepiece > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT\n",
    "    return f\"fairseq-generate {path_to_data} --gen-subset {subset} --source-lang {src} --target-lang {tgt} --path {model_checkpoint} --skip-invalid-size-inputs-valid-test --batch-size 128 --beam 5 --remove-bpe sentencepiece > {save_dir}\"\n",
    "\n",
    "\n",
    "def process_outputs(experiment_name: Path):\n",
    "    \"\"\"Outputs the processed outputs to the output directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment_name : Path\n",
    "        The name of the experiment\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, str]\n",
    "        A dictionary containing the keys `hyp`, `ref`, and `src` with the corresponding commands\n",
    "    \"\"\"\n",
    "    # cat $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT | grep -p ^H | sort -V | cut -f3- | sacremoses detokenize > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.hyp\n",
    "    # cat $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT | grep -p ^T | sort -V | cut -f2- | sacremoses detokenize > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.ref\n",
    "    # cat $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT | grep -p ^S | sort -V | cut -f2- | sacremoses detokenize > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.src\n",
    "    base = f\"cat {experiment_name} | grep -p {{grep}} | sort -V | cut -f{{cut}}- | sacremoses detokenize > {experiment_name}.{{ext}}\"\n",
    "    return {\n",
    "        \"hyp\": base.format(grep=\"^H\", cut=3, ext=\"hyp\"),\n",
    "        \"ref\": base.format(grep=\"^T\", cut=2, ext=\"ref\"),\n",
    "        \"src\": base.format(grep=\"^S\", cut=2, ext=\"src\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize_input(\n",
    "    source_file: Path, target_file: Path, src: str, tgt: str, sentencepiece_script: Path\n",
    ") -> Dict[str, Union[Path, List[str]]]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input files using sacremoses and sentencepiece\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_file : Path\n",
    "        The path to the source file\n",
    "    target_file : Path\n",
    "        The path to the target file\n",
    "    src : str\n",
    "        The source language\n",
    "    tgt : str\n",
    "        The target language\n",
    "    sentencepiece_script : Path\n",
    "        The path to the sentencepiece script\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Union[Path, List[str]]]\n",
    "        A dictionary containing the keys `tokenize_source_side`, `tokenize_target_side`, `encode`, and `output_files`\n",
    "\n",
    "    \"\"\"\n",
    "    # # tokenize train-mono, dev, test\n",
    "    # cat $src_train | sacremoses -l $SRC -j 4 normalize -c tokenize -a > $train_file.tok.$SRC\n",
    "    # cat $tgt_train | sacremoses -l $TGT -j 4 normalize -c tokenize -a > $train_file.tok.$TGT\n",
    "    # # separated for clarity\n",
    "    # python ./spm_encode.py --model=\"$spm\" \\\n",
    "    #     --output_format=piece \\\n",
    "    #     --inputs $train_file.tok.$SRC $train_file.tok.$TGT  \\\n",
    "    #     --outputs  $train_file.tok.spm.$SRC $train_file.tok.spm.$TGT\n",
    "    intermediate_files = (\n",
    "        str(source_file.with_suffix(\".tok.\" + src)),\n",
    "        str(target_file.with_suffix(\".tok.\" + tgt)),\n",
    "    )\n",
    "    output_files = (\n",
    "        source_file.with_suffix(\".tok.spm.\" + src),\n",
    "        source_file.with_suffix(\".tok.spm.\" + tgt),\n",
    "    )\n",
    "    return {\n",
    "        \"tokenize_source_side\":  f\"cat {source_file} | sacremoses -l {src} -j 4 normalize -c tokenize -a > {intermediate_files[0]}\",\n",
    "        \"tokenize_target_side\": f\"cat {target_file} | sacremoses -l {tgt} -j 4 normalize -c tokenize -a > {intermediate_files[1]}\",\n",
    "        \"encode\": [\n",
    "            \"python\",\n",
    "            str(sentencepiece_script),\n",
    "            \"--model\",\n",
    "            str(sentencepiece_script),\n",
    "            \"--output_format=piece\",\n",
    "            \"--inputs\",\n",
    "            *intermediate_files,\n",
    "            \"--outputs\",\n",
    "            str(output_files[0]),\n",
    "            str(output_files[1]),\n",
    "        ],\n",
    "        \"output_files\": output_files,\n",
    "    }\n",
    "\n",
    "\n",
    "def binarize_data(\n",
    "    src: str,\n",
    "    tgt: str,\n",
    "    src_dict,\n",
    "    tgt_dict,\n",
    "    train_prefix_file: Path,\n",
    "    valid_prefix_file: Path,\n",
    "    test_prefix_file: Path,\n",
    "    output_dir,\n",
    "    only_source=False,\n",
    "):\n",
    "    \"\"\"Binarizes the data. Note: if monolingual, use --only-source. Repeat in opposite direction if required, binary files are directional. Recommendation: Use different output directories for each direction.\"\"\"\n",
    "    # fairseq-preprocess \\\n",
    "    # --source-lang $SRC --target-lang $TGT \\\n",
    "    # --srcdict ./Data/it-mono/dict.$SRC.txt \\\n",
    "    # --tgtdict ./Data/it-mono/dict.$TGT.txt \\\n",
    "    # --trainpref $train_file.tok.spm \\\n",
    "    #     --validpref $dev_file.tok.spm \\\n",
    "    #     --testpref $test_file.tok.spm \\\n",
    "    # --destdir \"$(dirname $train_file)/bin\" \\\n",
    "    #     --thresholdtgt 0 --thresholdsrc 0 --workers 20 $only_source\n",
    "\n",
    "    # NOTE: if monolingual, --only-source\n",
    "    # repeat in opposite direction if required, binary files are directional\n",
    "    return [\n",
    "        \"fairseq-preprocess\",\n",
    "        \"--source-lang\",\n",
    "        src,\n",
    "        \"--target-lang\",\n",
    "        tgt,\n",
    "        \"--srcdict\",\n",
    "        str(src_dict),\n",
    "        \"--tgtdict\",\n",
    "        str(tgt_dict),\n",
    "        \"--trainpref\",\n",
    "        str(train_prefix_file.with_suffix(f\".tok.spm\")),\n",
    "        \"--validpref\",\n",
    "        str(valid_prefix_file.with_suffix(f\".tok.spm\")),\n",
    "        \"--testpref\",\n",
    "        str(test_prefix_file.with_suffix(f\".tok.spm\")),\n",
    "        \"--destdir\",\n",
    "        str(output_dir),\n",
    "        \"--thresholdtgt\",\n",
    "        \"0\",\n",
    "        \"--thresholdsrc\",\n",
    "        \"0\",\n",
    "        \"--workers\",\n",
    "        \"20\",\n",
    "        *([\"--only_source\"] if only_source else [])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "- Test base MODEL performance on the test set\n",
    "- Test base MODEL performance on the it-parallel dataset\n",
    "- Finetune the base MODEL on the it-parallel dataset and evaluate on both test sets\n",
    "\n",
    "### Results\n",
    "| Model |  News Corpus |  it-parallel |\n",
    "|-------|--------------|--------------|\n",
    "| Base  |  21.2      |  14.1      |\n",
    "| Finetune |  0.0000  |  0.0000      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"name\": \"BLEU\",\n",
      " \"score\": 14.1,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.1\",\n",
      " \"verbose_score\": \"43.1/18.4/9.6/5.2 (BP = 1.000 ratio = 1.032 hyp_len = 23650 ref_len = 22924)\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"no\",\n",
      " \"tok\": \"13a\",\n",
      " \"smooth\": \"exp\",\n",
      " \"version\": \"2.4.1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "base_model = model_dir / f\"big-{SRC}-{TGT}\" / \"checkpoint_best.pt\"\n",
    "\n",
    "experiment_name = f\"big-{SRC}-{TGT}-test-{it_mono}\"\n",
    "\n",
    "# Base evaluation on the news corpus first\n",
    "evaluate_news = process_outputs(test_folder / experiment_name)\n",
    "hyp_args, ref_args, src_args = evaluate_news.values()\n",
    "\n",
    "# Extract the hypothesis, reference, and source\n",
    "# res = subprocess.check_output(hyp_args, shell=True)\n",
    "# print(res)\n",
    "# res = subprocess.check_output(ref_args, shell=True)\n",
    "# print(res)\n",
    "# res = subprocess.check_output(src_args, shell=True)\n",
    "# print(res)\n",
    "\n",
    "args, file = get_bleu_score(test_folder / experiment_name)\n",
    "run_command(args)\n",
    "!cat $file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "\n",
      "INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'run_sanity_validation_steps': False}, 'common_eval': {'_name': None, 'path': '/Users/Matey/project/nlp2/Models/checkpoint_best-de-en.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'load_checkpoint_liberally': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-', 'force_override_max_positions': None}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': '/Users/Matey/project/nlp2/Data/train-euro-news-big/bin-de-en', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 8192, 'max_target_positions': 8192, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "\n",
      "INFO:fairseq.tasks.translation:[de] dictionary: 32016 types\n",
      "\n",
      "INFO:fairseq.tasks.translation:[en] dictionary: 32016 types\n",
      "\n",
      "INFO:fairseq_cli.generate:loading model(s) from /Users/Matey/project/nlp2/Models/checkpoint_best-de-en.pt\n",
      "\n",
      "INFO:fairseq.data.data_utils:loaded 3,003 examples from: /Users/Matey/project/nlp2/Data/train-euro-news-big/bin-de-en/test.de-en.de\n",
      "\n",
      "INFO:fairseq.data.data_utils:loaded 3,003 examples from: /Users/Matey/project/nlp2/Data/train-euro-news-big/bin-de-en/test.de-en.en\n",
      "\n",
      "INFO:fairseq.tasks.translation:/Users/Matey/project/nlp2/Data/train-euro-news-big/bin-de-en test de-en 3003 examples\n",
      "\n",
      "INFO:fairseq.tasks.fairseq_task:can_reuse_epoch_itr = True\n",
      "\n",
      "INFO:fairseq.tasks.fairseq_task:reuse_dataloader = True\n",
      "\n",
      "INFO:fairseq.tasks.fairseq_task:rebuild_batches = False\n",
      "\n",
      "INFO:fairseq.tasks.fairseq_task:creating new batches for epoch 1\n",
      "\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m args \u001b[38;5;241m=\u001b[39m generate_args(data_dir \u001b[38;5;241m/\u001b[39m news_dataset \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbin-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSRC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTGT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_checkpoint\u001b[38;5;241m=\u001b[39mbase_model_en_de, src\u001b[38;5;241m=\u001b[39mSRC, tgt\u001b[38;5;241m=\u001b[39mTGT, save_dir\u001b[38;5;241m=\u001b[39mtest_folder \u001b[38;5;241m/\u001b[39m model_name)\n\u001b[1;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mrun_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 5\u001b[0m, in \u001b[0;36mrun_command\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_command\u001b[39m(args: List[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[1;32m      3\u001b[0m         args, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m proc:\n\u001b[0;32m----> 5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proc\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate test results from news_dataset\n",
    "\n",
    "model_name = f\"big_de_en_{news_dataset}\"\n",
    "\n",
    "args = generate_args(data_dir / news_dataset / f\"bin-{SRC}-{TGT}\", \"test\", model_checkpoint=base_model_en_de, src=SRC, tgt=TGT, save_dir=test_folder / model_name)\n",
    "output = args.rsplit(\">\", 1)[-1].strip()\n",
    "print(args)\n",
    "run_command(args)\n",
    "\n",
    "evaluate_news = process_outputs(output)\n",
    "hyp_args, ref_args, src_args = evaluate_news.values()\n",
    "print(hyp_args, ref_args, src_args, sep='\\n')\n",
    "\n",
    "run_command(hyp_args)\n",
    "run_command(ref_args)\n",
    "run_command(src_args)\n",
    "\n",
    "output = Path(output)\n",
    "args, output = get_bleu_score(output)\n",
    "print(args)\n",
    "run_command(args)\n",
    "print_file(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairseq-preprocess --source-lang de --target-lang en --srcdict /Users/Matey/project/nlp2/Models/dict.de.txt --tgtdict /Users/Matey/project/nlp2/Models/dict.en.txt --trainpref /Users/Matey/project/nlp2/Data/it-parallel/train.tok.spm --validpref /Users/Matey/project/nlp2/Data/it-parallel/dev.tok.spm --testpref /Users/Matey/project/nlp2/Data/it-parallel/test.tok.spm --destdir /Users/Matey/project/nlp2/Data/it-parallel/bin-de-en --thresholdtgt 0 --thresholdsrc 0 --workers 20\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "\n",
      "INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', run_sanity_validation_steps=False, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref=None, validpref=None, testpref=None, align_suffix=None, destdir='data-bin', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/Matey/project/nlp2/.venv/bin/fairseq-preprocess\", line 8, in <module>\n",
      "\n",
      "    sys.exit(cli_main())\n",
      "\n",
      "             ^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/Matey/project/nlp2/.venv/lib/python3.11/site-packages/fairseq_cli/preprocess.py\", line 389, in cli_main\n",
      "\n",
      "    main(args)\n",
      "\n",
      "  File \"/Users/Matey/project/nlp2/.venv/lib/python3.11/site-packages/fairseq_cli/preprocess.py\", line 337, in main\n",
      "\n",
      "    assert (\n",
      "\n",
      "AssertionError: --trainpref must be set if --srcdict is not specified\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: 1 args: ['fairseq-preprocess', '--source-lang', 'de', '-...>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = binarize_data(\"de\", \"en\", dict_de, dict_en, train_prefix_file=data_dir / it_parallel / \"train\", valid_prefix_file=data_dir / it_parallel / \"dev\", test_prefix_file=data_dir / it_parallel / \"test\",  output_dir=data_dir / it_parallel / \"bin-de-en\")\n",
    "print(shlex.join(args))\n",
    "run_command(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', run_sanity_validation_steps=False, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='de', target_lang='en', trainpref='/Users/Matey/project/nlp2/Data/it-parallel/train.tok.spm', validpref='/Users/Matey/project/nlp2/Data/it-parallel/dev.tok.spm', testpref='/Users/Matey/project/nlp2/Data/it-parallel/test.tok.spm', align_suffix=None, destdir='/Users/Matey/project/nlp2/Data/it-parallel/bin-de-en', thresholdtgt=0, thresholdsrc=0, tgtdict='/Users/Matey/project/nlp2/Models/dict.en.txt', srcdict='/Users/Matey/project/nlp2/Models/dict.de.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=20, dict_only=False)\n",
      "INFO:fairseq_cli.preprocess:[de] Dictionary: 32016 types\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:[de] /Users/Matey/project/nlp2/Data/it-parallel/train.tok.spm.de: 20000 sents, 429230 tokens, 0.167% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[de] Dictionary: 32016 types\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:[de] /Users/Matey/project/nlp2/Data/it-parallel/dev.tok.spm.de: 2000 sents, 43767 tokens, 0.165% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[de] Dictionary: 32016 types\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:[de] /Users/Matey/project/nlp2/Data/it-parallel/test.tok.spm.de: 2000 sents, 41598 tokens, 0.231% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[en] Dictionary: 32016 types\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:[en] /Users/Matey/project/nlp2/Data/it-parallel/train.tok.spm.en: 20000 sents, 373732 tokens, 0.011% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[en] Dictionary: 32016 types\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:[en] /Users/Matey/project/nlp2/Data/it-parallel/dev.tok.spm.en: 2000 sents, 38263 tokens, 0.00261% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[en] Dictionary: 32016 types\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:[en] /Users/Matey/project/nlp2/Data/it-parallel/test.tok.spm.en: 2000 sents, 36224 tokens, 0.00276% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:Wrote preprocessed data to /Users/Matey/project/nlp2/Data/it-parallel/bin-de-en\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess --source-lang de --target-lang en --srcdict /Users/Matey/project/nlp2/Models/dict.de.txt --tgtdict /Users/Matey/project/nlp2/Models/dict.en.txt --trainpref /Users/Matey/project/nlp2/Data/it-parallel/train.tok.spm --validpref /Users/Matey/project/nlp2/Data/it-parallel/dev.tok.spm --testpref /Users/Matey/project/nlp2/Data/it-parallel/test.tok.spm --destdir /Users/Matey/project/nlp2/Data/it-parallel/bin-de-en --thresholdtgt 0 --thresholdsrc 0 --workers 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairseq-train /Users/Matey/project/nlp2/Data/it-parallel/bin-de-en --arch transformer_wmt_en_de --task translation --share-decoder-input-output-embed --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.1 --lr 0.0006 --lr-scheduler inverse_sqrt --warmup-updates 2500 --warmup-init-lr 1e-07 --stop-min-lr 1e-09 --dropout 0.3 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 8192 --max-update 10 --update-freq 8 --patience 10 --scoring sacrebleu --eval-bleu --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --save-interval-updates 2000 --validate-interval-updates 2000 --keep-best-checkpoints 1 --encoder-learned-pos --save-dir Models/big-de-en-ft-it-parallel --bpe sentencepiece\n"
     ]
    }
   ],
   "source": [
    "experiment_name = f\"big-{SRC}-{TGT}-ft-{it_parallel}\"\n",
    "train_args = get_train_model_args(data_dir / it_parallel / f\"bin-{SRC}-{TGT}\", experiment_name=experiment_name)\n",
    "print(' '.join(map(str, train_args)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-de-en and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is great, isn't it?\n"
     ]
    }
   ],
   "source": [
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
    "mname = \"facebook/wmt19-de-en\"\n",
    "tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
    "model = FSMTForConditionalGeneration.from_pretrained(mname)\n",
    "\n",
    "input = \"Maschinelles Lernen ist gro√üartig, oder?\"\n",
    "input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids)\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded) # Machine Learning is great, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_texts, tgt_texts, tokenizer, max_length=1024):\n",
    "        self.src_texts = src_texts\n",
    "        self.tgt_texts = tgt_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_texts[idx]\n",
    "        tgt_text = self.tgt_texts[idx]\n",
    "        return self.tokenizer(src_text, text_target=tgt_text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "it_parallel_src = data_dir / it_parallel / f\"train.{SRC}\"\n",
    "it_parallel_tgt = data_dir / it_parallel / f\"train.{TGT}\"\n",
    "# Create a dataset from the training parallel data\n",
    "with open(it_parallel_src) as f:\n",
    "    src = f.read().splitlines()\n",
    "with open(it_parallel_tgt) as f:\n",
    "    tgt = f.read().splitlines()\n",
    "train_dataset = TranslationDataset(src, tgt, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Create a dataset from the validation parallel data\n",
    "valid_src = data_dir / it_parallel / f\"dev.{SRC}\"\n",
    "valid_tgt = data_dir / it_parallel / f\"dev.{TGT}\"\n",
    "with open(valid_src) as f:\n",
    "    src = f.read().splitlines()\n",
    "with open(valid_tgt) as f:\n",
    "    tgt = f.read().splitlines()\n",
    "valid_dataset = TranslationDataset(src, tgt, tokenizer)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Create a dataset from the test parallel data\n",
    "test_src = data_dir / it_parallel / f\"test.{SRC}\"\n",
    "test_tgt = data_dir / it_parallel / f\"test.{TGT}\"\n",
    "with open(test_src) as f:\n",
    "    src = f.read().splitlines()\n",
    "with open(test_tgt) as f:\n",
    "    tgt = f.read().splitlines()\n",
    "    \n",
    "test_dataset = TranslationDataset(src, tgt, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,   957,   328,  9841,    23,     6,  3520,  1123,  1250,     9,\n",
      "             6, 19360,  1708,     2]])\n",
      "Method for the calculation of the prediction Method to calculate forecast\n",
      "tensor([[    2, 10708,  1463,     6,  2657,    47,   153,  4362,     5,    31,\n",
      "          7197,    22,    36,  6149,    15,   153, 16316,     9,     6,  5617,\n",
      "          2657,    14,     5,   512,   109,   157,   427,     6,    18,  2152,\n",
      "            41,    19, 16228,  4670,     5,     2]])\n",
      "Reduces the speed by one unit. The minimum is 2 units (one fifth of the normal speed). You can also use the - key as a shortcut. Decreases the game speed by one unit, down to a minimum of 2 units (a fifth of normal speed). You can use the - key as a shortcut.\n",
      "tensor([[   2, 1337,  400,  359, 9543,  167,  780, 2266,    2]])\n",
      "Piotr Szymanski Piotr Szymanski\n",
      "tensor([[    2,    50,  1762,  4085, 12075,    47,     2]])\n",
      "Sort & sort by Sort & By\n",
      "tensor([[2, 2]])\n",
      " umbrello; Authors\n",
      "tensor([[   2, 1838,  297, 4206,  483,  546,    2]])\n",
      "Disable Netbios Disable netbios\n",
      "tensor([[   2, 1396,  648, 1445,  116, 1562,    2]])\n",
      "Paste Clipboard Paste clipboard\n",
      "tensor([[   2, 5577, 5907,    2]])\n",
      "Guidance Guides\n",
      "tensor([[   2, 4689,    6, 1921,   10, 1969,  141,    2]])\n",
      "Read the Video-DVD... Rip Video DVD...\n",
      "tensor([[   2, 1838,  297,   24, 1407,   69, 7841,  460,   24,    2]])\n",
      "Disable \"Fix Content\" Switching off \"fixed content\"\n",
      "tensor([[    2,   805,   297,    57,  1045,   297,     6,   427,     9,  1673,\n",
      "         14372,   465,   270,    42,  1794,     7,    19,  2723,  3582,     5,\n",
      "             2]])\n",
      "Enable or disable the use of webseeds if they are present in a torrent. Enable or disable the use of webseeds when they are present in a torrent.\n",
      "tensor([[    2,   597, 13353, 11328,   193,   764,    11,  1168,  5712,     2]])\n",
      "Monthly Budgeted and Actual Monthly Budgeted vs. Actual\n",
      "tensor([[    2,    31,   753, 13766,  8753,     7,     6,  8513, 17806,    42,\n",
      "         13408,  5336,    16,   624,   465,    19,  8513,    22, 23933,     6,\n",
      "           753,  4708,     5,     2]])\n",
      "The sockets listed in the write array are monitored to see if a write is blocking the socket. The sockets listed in the write array will be watched to see if a write will not block.\n",
      "tensor([[  2, 605, 279,   4, 260, 315,   2]])\n",
      "exp, ln exp, ln\n",
      "tensor([[   2, 6221,    5,    2]])\n",
      "document. User data\n",
      "tensor([[    2,   253,    22,     6,  2080,     9, 16803,  5210,     5,   284,\n",
      "           109,    44,   283,    15, 13236,  1287,    10,    16,    10,  1287,\n",
      "          2866,  5210,    14,    57, 21163,    15, 13236,    72,   928,  1552,\n",
      "          2866,  5210,    14,   151,    31, 10667, 13984,    77,  6577,    63,\n",
      "           112,  2080,    16,   427,     5,  2370, 11341,     4,   283,    22,\n",
      "           675,     5,     2]])\n",
      "This is the type of tunnel device. It can be do (virtual point-to-point network device) or tap (virtual Ethernet network device).The administrator will tell you which type to use. By default, do is used. This is the type of your tunnel device. It can be tun (virtual Point-to-Point network device) or tap (virtual Ethernet network device). Your administrator will tell you which you have to use. Default is to use the tun device.\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    tokens = batch.input_ids\n",
    "    for i in range(tokens.size(0)):\n",
    "        output = model.generate(tokens[i])\n",
    "        print(output)\n",
    "        src_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        true_label = tokenizer.decode(batch.labels[i][0], skip_special_tokens=True)\n",
    "        print(src_text, true_label)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
