{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Backtranslation\n",
    "Let's define some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "from typing import Dict, List, Union, Tuple\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "SRC = \"de\"\n",
    "TGT = \"en\"\n",
    "\n",
    "\n",
    "cwd = Path.cwd()\n",
    "data_dir = cwd / \"Data\"\n",
    "model_dir = cwd / \"Models\"\n",
    "\n",
    "it_parallel = \"it-parallel\"\n",
    "news_dataset = \"train-euro-news-big\"\n",
    "it_mono = \"it-mono\"\n",
    "\n",
    "test_folder = cwd / \"tests\"\n",
    "\n",
    "sentencepiece_script = cwd / \"spm_encode.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "base_model_en_de = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"checkpoint_best-en-de.pt\", local_dir=\"Models/hugging_face\")\n",
    "base_model_en_de = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"checkpoint_best-de-en.pt\", local_dir=\"Models/hugging_face\")\n",
    "dict_de = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"dict.de.txt\", local_dir=\"Models/hugging_face\")\n",
    "dict_en = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"dict.en.txt\", local_dir=\"Models/hugging_face\")\n",
    "sentencepiece_model = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"spm.model\", local_dir=\"Models/hugging_face\")\n",
    "\n",
    "from tokenizers import SentencePieceUnigramTokenizer\n",
    "tokenizer = SentencePieceUnigramTokenizer.from_spm(\n",
    "    sentencepiece_model\n",
    ")\n",
    "\n",
    "tokens = tokenizer.encode(\"Hello, how are you?\")\n",
    "tokenizer.decode(tokens.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(args: List[str]):\n",
    "    with subprocess.Popen(\n",
    "        args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, shell=True\n",
    "    ) as proc:\n",
    "        for line in proc.stdout:\n",
    "            print(line)\n",
    "    return proc\n",
    "\n",
    "\n",
    "def get_train_model_args(\n",
    "    path_to_data,\n",
    "    arch=\"transformer_wmt_en_de\",\n",
    "    max_update=10,\n",
    "    model_dir=\"Models\",\n",
    "    experiment_name=\"test-de-en\",\n",
    "    lr=6e-4,\n",
    "):\n",
    "    return [\n",
    "        \"fairseq-train\",\n",
    "        path_to_data,\n",
    "        \"--arch\",\n",
    "        arch,\n",
    "        \"--task translation\",\n",
    "        \"--share-decoder-input-output-embed\",\n",
    "        \"--optimizer adam\",\n",
    "        \"--adam-betas '(0.9, 0.98)'\",\n",
    "        \"--clip-norm 0.1\",\n",
    "        \"--lr\",\n",
    "        lr,\n",
    "        \"--lr-scheduler inverse_sqrt\",\n",
    "        \"--warmup-updates 2500\",\n",
    "        \"--warmup-init-lr 1e-07\",\n",
    "        \"--stop-min-lr 1e-09\",\n",
    "        \"--dropout 0.3\",\n",
    "        \"--weight-decay 0.0001\",\n",
    "        \"--criterion label_smoothed_cross_entropy\",\n",
    "        \"--label-smoothing 0.1\",\n",
    "        \"--max-tokens 8192\",\n",
    "        \"--max-update\",\n",
    "        max_update,\n",
    "        \"--update-freq 8\",\n",
    "        \"--patience 10\",\n",
    "        \"--scoring sacrebleu\",\n",
    "        \"--eval-bleu\",\n",
    "        '--eval-bleu-args \\'{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}\\'',\n",
    "        \"--eval-bleu-detok moses\",\n",
    "        \"--eval-bleu-remove-bpe\",\n",
    "        \"--eval-bleu-print-samples\",\n",
    "        \"--best-checkpoint-metric bleu\",\n",
    "        \"--maximize-best-checkpoint-metric\",\n",
    "        \"--save-interval-updates 2000\",\n",
    "        \"--validate-interval-updates 2000\",\n",
    "        \"--keep-best-checkpoints 1\",\n",
    "        \"--encoder-learned-pos\",\n",
    "        \"--save-dir\",\n",
    "        model_dir + \"/\" + experiment_name,\n",
    "        \"--bpe sentencepiece\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_bleu_score(experiment: Path):\n",
    "    \"\"\"Returns the command to calculate the BLEU score. Final path component is the result file.\"\"\"\n",
    "    # cat $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.hyp | sacrebleu $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.ref -m bleu > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.sacrebleu\n",
    "    return \"cat {0}.hyp | sacrebleu {0}.ref -m bleu > {0}.sacrebleu\".format(\n",
    "        experiment\n",
    "    ), experiment.with_suffix(\".sacrebleu\")\n",
    "\n",
    "\n",
    "# [\n",
    "\n",
    "#         \"cat\",\n",
    "#         str(experiment.with_suffix(\".hyp\")),\n",
    "#         \"| sacrebleu\",\n",
    "#         str(experiment.with_suffix(\".ref\")),\n",
    "#         \"-m\",\n",
    "#         \"bleu >\",\n",
    "#        str( experiment.with_suffix(\".sacrebleu\")),\n",
    "#     ]\n",
    "\n",
    "\n",
    "def generate_args(\n",
    "    path_to_data: str | Path,\n",
    "    subset: str,\n",
    "    src: str,\n",
    "    tgt: str,\n",
    "    model_checkpoint: str | Path,\n",
    "    save_dir: str,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates the arguments for the fairseq-generate command\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_data : str\n",
    "        Path to the data directory (the data must be binarized)\n",
    "    subset : str\n",
    "        The subset to generate the outputs for (e.g., test)\n",
    "    src : str\n",
    "        The source language\n",
    "    tgt : str\n",
    "        The target language\n",
    "    model_checkpoint : str\n",
    "        The path to the model checkpoint\n",
    "    \"\"\"\n",
    "    # fairseq-generate Data/$TEST/bin \\\n",
    "    #  --gen-subset test --source-lang $SRC --target-lang $TGT \\\n",
    "    #  --path./Models/$MODEL/checkpoint_best.pt \\\n",
    "    #  --skip-invalid-size-inputs-valid-test \\\n",
    "    #  --batch-size 128 --beam 5 --remove-bpe sentencepiece > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT\n",
    "    return f\"fairseq-generate {path_to_data} --gen-subset {subset} --source-lang {src} --target-lang {tgt} --path {model_checkpoint} --skip-invalid-size-inputs-valid-test --batch-size 128 --beam 5 --remove-bpe sentencepiece > {save_dir}\"\n",
    "\n",
    "\n",
    "def process_outputs(experiment_name: Path):\n",
    "    \"\"\"Outputs the processed outputs to the output directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment_name : Path\n",
    "        The name of the experiment\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, str]\n",
    "        A dictionary containing the keys `hyp`, `ref`, and `src` with the corresponding commands\n",
    "    \"\"\"\n",
    "    # cat $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT | grep -p ^H | sort -V | cut -f3- | sacremoses detokenize > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.hyp\n",
    "    # cat $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT | grep -p ^T | sort -V | cut -f2- | sacremoses detokenize > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.ref\n",
    "    # cat $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT | grep -p ^S | sort -V | cut -f2- | sacremoses detokenize > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.src\n",
    "    base = f\"cat {experiment_name} | grep -p {{grep}} | sort -V | cut -f{{cut}}- | sacremoses detokenize > {experiment_name}.{{ext}}\"\n",
    "    return {\n",
    "        \"hyp\": base.format(grep=\"^H\", cut=3, ext=\"hyp\"),\n",
    "        \"ref\": base.format(grep=\"^T\", cut=2, ext=\"ref\"),\n",
    "        \"src\": base.format(grep=\"^S\", cut=2, ext=\"src\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize_input(\n",
    "    source_file: Path, target_file: Path, src: str, tgt: str, sentencepiece_script: Path\n",
    ") -> Dict[str, Union[Path, List[str]]]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input files using sacremoses and sentencepiece\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_file : Path\n",
    "        The path to the source file\n",
    "    target_file : Path\n",
    "        The path to the target file\n",
    "    src : str\n",
    "        The source language\n",
    "    tgt : str\n",
    "        The target language\n",
    "    sentencepiece_script : Path\n",
    "        The path to the sentencepiece script\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Union[Path, List[str]]]\n",
    "        A dictionary containing the keys `tokenize_source_side`, `tokenize_target_side`, `encode`, and `output_files`\n",
    "\n",
    "    \"\"\"\n",
    "    # # tokenize train-mono, dev, test\n",
    "    # cat $src_train | sacremoses -l $SRC -j 4 normalize -c tokenize -a > $train_file.tok.$SRC\n",
    "    # cat $tgt_train | sacremoses -l $TGT -j 4 normalize -c tokenize -a > $train_file.tok.$TGT\n",
    "    # # separated for clarity\n",
    "    # python ./spm_encode.py --model=\"$spm\" \\\n",
    "    #     --output_format=piece \\\n",
    "    #     --inputs $train_file.tok.$SRC $train_file.tok.$TGT  \\\n",
    "    #     --outputs  $train_file.tok.spm.$SRC $train_file.tok.spm.$TGT\n",
    "    intermediate_files = (\n",
    "        str(source_file.with_suffix(\".tok.\" + src)),\n",
    "        str(target_file.with_suffix(\".tok.\" + tgt)),\n",
    "    )\n",
    "    output_files = (\n",
    "        source_file.with_suffix(\".tok.spm.\" + src),\n",
    "        source_file.with_suffix(\".tok.spm.\" + tgt),\n",
    "    )\n",
    "    return {\n",
    "        \"tokenize_source_side\":  f\"cat {source_file} | sacremoses -l {src} -j 4 normalize -c tokenize -a > {intermediate_files[0]}\",\n",
    "        \"tokenize_target_side\": f\"cat {target_file} | sacremoses -l {tgt} -j 4 normalize -c tokenize -a > {intermediate_files[1]}\",\n",
    "        \"encode\": [\n",
    "            \"python\",\n",
    "            str(sentencepiece_script),\n",
    "            \"--model\",\n",
    "            str(sentencepiece_script),\n",
    "            \"--output_format=piece\",\n",
    "            \"--inputs\",\n",
    "            *intermediate_files,\n",
    "            \"--outputs\",\n",
    "            str(output_files[0]),\n",
    "            str(output_files[1]),\n",
    "        ],\n",
    "        \"output_files\": output_files,\n",
    "    }\n",
    "\n",
    "\n",
    "def binarize_data(\n",
    "    src: str,\n",
    "    tgt: str,\n",
    "    src_dict,\n",
    "    tgt_dict,\n",
    "    train_prefix_file: Path,\n",
    "    valid_prefix_file: Path,\n",
    "    test_prefix_file: Path,\n",
    "    output_dir,\n",
    "    only_source=False,\n",
    "):\n",
    "    \"\"\"Binarizes the data. Note: if monolingual, use --only-source. Repeat in opposite direction if required, binary files are directional. Recommendation: Use different output directories for each direction.\"\"\"\n",
    "    # fairseq-preprocess \\\n",
    "    # --source-lang $SRC --target-lang $TGT \\\n",
    "    # --srcdict ./Data/it-mono/dict.$SRC.txt \\\n",
    "    # --tgtdict ./Data/it-mono/dict.$TGT.txt \\\n",
    "    # --trainpref $train_file.tok.spm \\\n",
    "    #     --validpref $dev_file.tok.spm \\\n",
    "    #     --testpref $test_file.tok.spm \\\n",
    "    # --destdir \"$(dirname $train_file)/bin\" \\\n",
    "    #     --thresholdtgt 0 --thresholdsrc 0 --workers 20 $only_source\n",
    "\n",
    "    # NOTE: if monolingual, --only-source\n",
    "    # repeat in opposite direction if required, binary files are directional\n",
    "    return [\n",
    "        \"fairseq_preprocess\",\n",
    "        \"--source-lang\",\n",
    "        src,\n",
    "        \"--target-lang\",\n",
    "        tgt,\n",
    "        \"--srcdict\",\n",
    "        str(src_dict),\n",
    "        \"--tgtdict\",\n",
    "        str(tgt_dict),\n",
    "        \"--trainpref\",\n",
    "        str(train_prefix_file.with_suffix(\".tok.spm\")),\n",
    "        \"--validpref\",\n",
    "        str(valid_prefix_file.with_suffix(\".tok.spm\")),\n",
    "        \"--testpref\",\n",
    "        str(test_prefix_file.with_suffix(\".tok.spm\")),\n",
    "        \"--destdir\",\n",
    "        str(output_dir),\n",
    "        \"--thresholdtgt\",\n",
    "        \"0\",\n",
    "        \"--thresholdsrc\",\n",
    "        \"0\",\n",
    "        \"--workers\",\n",
    "        \"20\",\n",
    "        \"--only-source\" if only_source else \"\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "- Test base MODEL performance on the test set\n",
    "- Test base MODEL performance on the it-parallel dataset\n",
    "- Finetune the base MODEL on the it-parallel dataset and evaluate on both test sets\n",
    "\n",
    "### Results\n",
    "| Model |  News Corpus |  it-parallel |\n",
    "|-------|--------------|--------------|\n",
    "| Base  |  0.0000      |  0.0000      |\n",
    "| Finetune |  0.0000  |  0.0000      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"name\": \"BLEU\",\n",
      " \"score\": 14.1,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.1\",\n",
      " \"verbose_score\": \"43.1/18.4/9.6/5.2 (BP = 1.000 ratio = 1.032 hyp_len = 23650 ref_len = 22924)\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"no\",\n",
      " \"tok\": \"13a\",\n",
      " \"smooth\": \"exp\",\n",
      " \"version\": \"2.4.1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "base_model = model_dir / f\"big-{SRC}-{TGT}\" / \"checkpoint_best.pt\"\n",
    "\n",
    "experiment_name = f\"big-{SRC}-{TGT}-test-{it_mono}\"\n",
    "\n",
    "# Base evaluation on the news corpus first\n",
    "evaluate_news = process_outputs(test_folder / experiment_name)\n",
    "hyp_args, ref_args, src_args = evaluate_news.values()\n",
    "\n",
    "# Extract the hypothesis, reference, and source\n",
    "# res = subprocess.check_output(hyp_args, shell=True)\n",
    "# print(res)\n",
    "# res = subprocess.check_output(ref_args, shell=True)\n",
    "# print(res)\n",
    "# res = subprocess.check_output(src_args, shell=True)\n",
    "# print(res)\n",
    "\n",
    "args, file = get_bleu_score(test_folder / experiment_name)\n",
    "run_command(args)\n",
    "!cat $file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
