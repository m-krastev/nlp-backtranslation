{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Backtranslation\n",
    "Let's define some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "from typing import Dict, List, Union, Tuple\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "SRC = \"de\"\n",
    "TGT = \"en\"\n",
    "\n",
    "\n",
    "cwd = Path.cwd()\n",
    "data_dir = cwd / \"Data\"\n",
    "model_dir = cwd / \"Models\" / \"hugging_face\"\n",
    "\n",
    "it_parallel = \"it-parallel\"\n",
    "news_dataset = \"train-euro-news-big\"\n",
    "it_mono = \"it-mono\"\n",
    "\n",
    "test_folder = cwd / \"tests\"\n",
    "\n",
    "sentencepiece_script = cwd / \"spm_encode.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, how are you?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "base_model_en_de = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"checkpoint_best-en-de.pt\", local_dir=model_dir)\n",
    "base_model_en_de = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"checkpoint_best-de-en.pt\", local_dir=model_dir)\n",
    "dict_de = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"dict.de.txt\", local_dir=model_dir)\n",
    "dict_en = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"dict.en.txt\", local_dir=model_dir)\n",
    "sentencepiece_model = hf_hub_download(\"rinto/transformer_wmt_en_de\", \"spm.model\", local_dir=model_dir)\n",
    "\n",
    "from tokenizers import SentencePieceUnigramTokenizer\n",
    "tokenizer = SentencePieceUnigramTokenizer.from_spm(\n",
    "    sentencepiece_model\n",
    ")\n",
    "\n",
    "tokens = tokenizer.encode(\"Hello, how are you?\")\n",
    "tokenizer.decode(tokens.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(args: List[str]):\n",
    "    with subprocess.Popen(\n",
    "        args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, shell=True\n",
    "    ) as proc:\n",
    "        for line in proc.stdout:\n",
    "            print(line)\n",
    "    return proc\n",
    "\n",
    "def print_file(path: Path):\n",
    "    with path.open() as f:\n",
    "        print(f.read())\n",
    "\n",
    "def get_train_model_args(\n",
    "    path_to_data,\n",
    "    arch=\"transformer_wmt_en_de\",\n",
    "    max_update=10,\n",
    "    model_dir=\"Models\",\n",
    "    experiment_name=\"test-de-en\",\n",
    "    lr=6e-4,\n",
    "):\n",
    "    return [\n",
    "        \"fairseq-train\",\n",
    "        path_to_data,\n",
    "        \"--arch\",\n",
    "        arch,\n",
    "        \"--task translation\",\n",
    "        \"--share-decoder-input-output-embed\",\n",
    "        \"--optimizer adam\",\n",
    "        \"--adam-betas '(0.9, 0.98)'\",\n",
    "        \"--clip-norm 0.1\",\n",
    "        \"--lr\",\n",
    "        lr,\n",
    "        \"--lr-scheduler inverse_sqrt\",\n",
    "        \"--warmup-updates 2500\",\n",
    "        \"--warmup-init-lr 1e-07\",\n",
    "        \"--stop-min-lr 1e-09\",\n",
    "        \"--dropout 0.3\",\n",
    "        \"--weight-decay 0.0001\",\n",
    "        \"--criterion label_smoothed_cross_entropy\",\n",
    "        \"--label-smoothing 0.1\",\n",
    "        \"--max-tokens 8192\",\n",
    "        \"--max-update\",\n",
    "        max_update,\n",
    "        \"--update-freq 8\",\n",
    "        \"--patience 10\",\n",
    "        \"--scoring sacrebleu\",\n",
    "        \"--eval-bleu\",\n",
    "        '--eval-bleu-args \\'{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}\\'',\n",
    "        \"--eval-bleu-detok moses\",\n",
    "        \"--eval-bleu-remove-bpe\",\n",
    "        \"--eval-bleu-print-samples\",\n",
    "        \"--best-checkpoint-metric bleu\",\n",
    "        \"--maximize-best-checkpoint-metric\",\n",
    "        \"--save-interval-updates 2000\",\n",
    "        \"--validate-interval-updates 2000\",\n",
    "        \"--keep-best-checkpoints 1\",\n",
    "        \"--encoder-learned-pos\",\n",
    "        \"--save-dir\",\n",
    "        model_dir + \"/\" + experiment_name,\n",
    "        \"--bpe sentencepiece\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_bleu_score(experiment: Path):\n",
    "    \"\"\"Returns the command to calculate the BLEU score. Final path component is the result file.\"\"\"\n",
    "    # cat $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.hyp | sacrebleu $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.ref -m bleu > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.sacrebleu\n",
    "    return \"cat {0}.hyp | sacrebleu {0}.ref -m bleu > {0}.sacrebleu\".format(\n",
    "        experiment\n",
    "    ), experiment.with_suffix(\".sacrebleu\")\n",
    "\n",
    "\n",
    "# [\n",
    "\n",
    "#         \"cat\",\n",
    "#         str(experiment.with_suffix(\".hyp\")),\n",
    "#         \"| sacrebleu\",\n",
    "#         str(experiment.with_suffix(\".ref\")),\n",
    "#         \"-m\",\n",
    "#         \"bleu >\",\n",
    "#        str( experiment.with_suffix(\".sacrebleu\")),\n",
    "#     ]\n",
    "\n",
    "\n",
    "def generate_args(\n",
    "    path_to_data: str | Path,\n",
    "    subset: str,\n",
    "    src: str,\n",
    "    tgt: str,\n",
    "    model_checkpoint: str | Path,\n",
    "    save_dir: str,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates the arguments for the fairseq-generate command\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_data : str\n",
    "        Path to the data directory (the data must be binarized)\n",
    "    subset : str\n",
    "        The subset to generate the outputs for (e.g., test)\n",
    "    src : str\n",
    "        The source language\n",
    "    tgt : str\n",
    "        The target language\n",
    "    model_checkpoint : str\n",
    "        The path to the model checkpoint\n",
    "    \"\"\"\n",
    "    # fairseq-generate Data/$TEST/bin \\\n",
    "    #  --gen-subset test --source-lang $SRC --target-lang $TGT \\\n",
    "    #  --path./Models/$MODEL/checkpoint_best.pt \\\n",
    "    #  --skip-invalid-size-inputs-valid-test \\\n",
    "    #  --batch-size 128 --beam 5 --remove-bpe sentencepiece > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT\n",
    "    return f\"fairseq-generate {path_to_data} --gen-subset {subset} --source-lang {src} --target-lang {tgt} --path {model_checkpoint} --skip-invalid-size-inputs-valid-test --batch-size 128 --beam 5 --remove-bpe sentencepiece > {save_dir}\"\n",
    "\n",
    "\n",
    "def process_outputs(experiment_name: Path):\n",
    "    \"\"\"Outputs the processed outputs to the output directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment_name : Path\n",
    "        The name of the experiment\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, str]\n",
    "        A dictionary containing the keys `hyp`, `ref`, and `src` with the corresponding commands\n",
    "    \"\"\"\n",
    "    # cat $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT | grep -p ^H | sort -V | cut -f3- | sacremoses detokenize > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.hyp\n",
    "    # cat $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT | grep -p ^T | sort -V | cut -f2- | sacremoses detokenize > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.ref\n",
    "    # cat $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT | grep -p ^S | sort -V | cut -f2- | sacremoses detokenize > $OUTPUT_DIR/$MODEL.test-$TEST-$SRC-$TGT.src\n",
    "    base = f\"cat {experiment_name} | grep -p {{grep}} | sort -V | cut -f{{cut}}- | sacremoses detokenize > {experiment_name}.{{ext}}\"\n",
    "    return {\n",
    "        \"hyp\": base.format(grep=\"^H\", cut=3, ext=\"hyp\"),\n",
    "        \"ref\": base.format(grep=\"^T\", cut=2, ext=\"ref\"),\n",
    "        \"src\": base.format(grep=\"^S\", cut=2, ext=\"src\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize_input(\n",
    "    source_file: Path, target_file: Path, src: str, tgt: str, sentencepiece_script: Path\n",
    ") -> Dict[str, Union[Path, List[str]]]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input files using sacremoses and sentencepiece\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_file : Path\n",
    "        The path to the source file\n",
    "    target_file : Path\n",
    "        The path to the target file\n",
    "    src : str\n",
    "        The source language\n",
    "    tgt : str\n",
    "        The target language\n",
    "    sentencepiece_script : Path\n",
    "        The path to the sentencepiece script\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Union[Path, List[str]]]\n",
    "        A dictionary containing the keys `tokenize_source_side`, `tokenize_target_side`, `encode`, and `output_files`\n",
    "\n",
    "    \"\"\"\n",
    "    # # tokenize train-mono, dev, test\n",
    "    # cat $src_train | sacremoses -l $SRC -j 4 normalize -c tokenize -a > $train_file.tok.$SRC\n",
    "    # cat $tgt_train | sacremoses -l $TGT -j 4 normalize -c tokenize -a > $train_file.tok.$TGT\n",
    "    # # separated for clarity\n",
    "    # python ./spm_encode.py --model=\"$spm\" \\\n",
    "    #     --output_format=piece \\\n",
    "    #     --inputs $train_file.tok.$SRC $train_file.tok.$TGT  \\\n",
    "    #     --outputs  $train_file.tok.spm.$SRC $train_file.tok.spm.$TGT\n",
    "    intermediate_files = (\n",
    "        str(source_file.with_suffix(\".tok.\" + src)),\n",
    "        str(target_file.with_suffix(\".tok.\" + tgt)),\n",
    "    )\n",
    "    output_files = (\n",
    "        source_file.with_suffix(\".tok.spm.\" + src),\n",
    "        source_file.with_suffix(\".tok.spm.\" + tgt),\n",
    "    )\n",
    "    return {\n",
    "        \"tokenize_source_side\":  f\"cat {source_file} | sacremoses -l {src} -j 4 normalize -c tokenize -a > {intermediate_files[0]}\",\n",
    "        \"tokenize_target_side\": f\"cat {target_file} | sacremoses -l {tgt} -j 4 normalize -c tokenize -a > {intermediate_files[1]}\",\n",
    "        \"encode\": [\n",
    "            \"python\",\n",
    "            str(sentencepiece_script),\n",
    "            \"--model\",\n",
    "            str(sentencepiece_script),\n",
    "            \"--output_format=piece\",\n",
    "            \"--inputs\",\n",
    "            *intermediate_files,\n",
    "            \"--outputs\",\n",
    "            str(output_files[0]),\n",
    "            str(output_files[1]),\n",
    "        ],\n",
    "        \"output_files\": output_files,\n",
    "    }\n",
    "\n",
    "\n",
    "def binarize_data(\n",
    "    src: str,\n",
    "    tgt: str,\n",
    "    src_dict,\n",
    "    tgt_dict,\n",
    "    train_prefix_file: Path,\n",
    "    valid_prefix_file: Path,\n",
    "    test_prefix_file: Path,\n",
    "    output_dir,\n",
    "    only_source=False,\n",
    "):\n",
    "    \"\"\"Binarizes the data. Note: if monolingual, use --only-source. Repeat in opposite direction if required, binary files are directional. Recommendation: Use different output directories for each direction.\"\"\"\n",
    "    # fairseq-preprocess \\\n",
    "    # --source-lang $SRC --target-lang $TGT \\\n",
    "    # --srcdict ./Data/it-mono/dict.$SRC.txt \\\n",
    "    # --tgtdict ./Data/it-mono/dict.$TGT.txt \\\n",
    "    # --trainpref $train_file.tok.spm \\\n",
    "    #     --validpref $dev_file.tok.spm \\\n",
    "    #     --testpref $test_file.tok.spm \\\n",
    "    # --destdir \"$(dirname $train_file)/bin\" \\\n",
    "    #     --thresholdtgt 0 --thresholdsrc 0 --workers 20 $only_source\n",
    "\n",
    "    # NOTE: if monolingual, --only-source\n",
    "    # repeat in opposite direction if required, binary files are directional\n",
    "    return [\n",
    "        \"fairseq-preprocess\",\n",
    "        \"--source-lang\",\n",
    "        src,\n",
    "        \"--target-lang\",\n",
    "        tgt,\n",
    "        \"--srcdict\",\n",
    "        str(src_dict),\n",
    "        \"--tgtdict\",\n",
    "        str(tgt_dict),\n",
    "        \"--trainpref\",\n",
    "        str(train_prefix_file.with_suffix(f\".tok.spm\")),\n",
    "        \"--validpref\",\n",
    "        str(valid_prefix_file.with_suffix(f\".tok.spm\")),\n",
    "        \"--testpref\",\n",
    "        str(test_prefix_file.with_suffix(f\".tok.spm\")),\n",
    "        \"--destdir\",\n",
    "        str(output_dir),\n",
    "        \"--thresholdtgt\",\n",
    "        \"0\",\n",
    "        \"--thresholdsrc\",\n",
    "        \"0\",\n",
    "        \"--workers\",\n",
    "        \"20\",\n",
    "        *([\"--only_source\"] if only_source else [])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "- Test base MODEL performance on the test set\n",
    "- Test base MODEL performance on the it-parallel dataset\n",
    "- Finetune the base MODEL on the it-parallel dataset and evaluate on both test sets\n",
    "\n",
    "### Results\n",
    "| Model |  News Corpus |  it-parallel |\n",
    "|-------|--------------|--------------|\n",
    "| Base  |  21.2      |  0.0000      |\n",
    "| Finetune |  0.0000  |  0.0000      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"name\": \"BLEU\",\n",
      " \"score\": 14.1,\n",
      " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.1\",\n",
      " \"verbose_score\": \"43.1/18.4/9.6/5.2 (BP = 1.000 ratio = 1.032 hyp_len = 23650 ref_len = 22924)\",\n",
      " \"nrefs\": \"1\",\n",
      " \"case\": \"mixed\",\n",
      " \"eff\": \"no\",\n",
      " \"tok\": \"13a\",\n",
      " \"smooth\": \"exp\",\n",
      " \"version\": \"2.4.1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "base_model = model_dir / f\"big-{SRC}-{TGT}\" / \"checkpoint_best.pt\"\n",
    "\n",
    "experiment_name = f\"big-{SRC}-{TGT}-test-{it_mono}\"\n",
    "\n",
    "# Base evaluation on the news corpus first\n",
    "evaluate_news = process_outputs(test_folder / experiment_name)\n",
    "hyp_args, ref_args, src_args = evaluate_news.values()\n",
    "\n",
    "# Extract the hypothesis, reference, and source\n",
    "# res = subprocess.check_output(hyp_args, shell=True)\n",
    "# print(res)\n",
    "# res = subprocess.check_output(ref_args, shell=True)\n",
    "# print(res)\n",
    "# res = subprocess.check_output(src_args, shell=True)\n",
    "# print(res)\n",
    "\n",
    "args, file = get_bleu_score(test_folder / experiment_name)\n",
    "run_command(args)\n",
    "!cat $file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "\n",
      "INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'run_sanity_validation_steps': False}, 'common_eval': {'_name': None, 'path': '/Users/Matey/project/nlp2/Models/checkpoint_best-de-en.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'load_checkpoint_liberally': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-', 'force_override_max_positions': None}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': '/Users/Matey/project/nlp2/Data/train-euro-news-big/bin-de-en', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 8192, 'max_target_positions': 8192, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "\n",
      "INFO:fairseq.tasks.translation:[de] dictionary: 32016 types\n",
      "\n",
      "INFO:fairseq.tasks.translation:[en] dictionary: 32016 types\n",
      "\n",
      "INFO:fairseq_cli.generate:loading model(s) from /Users/Matey/project/nlp2/Models/checkpoint_best-de-en.pt\n",
      "\n",
      "INFO:fairseq.data.data_utils:loaded 3,003 examples from: /Users/Matey/project/nlp2/Data/train-euro-news-big/bin-de-en/test.de-en.de\n",
      "\n",
      "INFO:fairseq.data.data_utils:loaded 3,003 examples from: /Users/Matey/project/nlp2/Data/train-euro-news-big/bin-de-en/test.de-en.en\n",
      "\n",
      "INFO:fairseq.tasks.translation:/Users/Matey/project/nlp2/Data/train-euro-news-big/bin-de-en test de-en 3003 examples\n",
      "\n",
      "INFO:fairseq.tasks.fairseq_task:can_reuse_epoch_itr = True\n",
      "\n",
      "INFO:fairseq.tasks.fairseq_task:reuse_dataloader = True\n",
      "\n",
      "INFO:fairseq.tasks.fairseq_task:rebuild_batches = False\n",
      "\n",
      "INFO:fairseq.tasks.fairseq_task:creating new batches for epoch 1\n",
      "\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m args \u001b[38;5;241m=\u001b[39m generate_args(data_dir \u001b[38;5;241m/\u001b[39m news_dataset \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbin-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSRC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTGT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_checkpoint\u001b[38;5;241m=\u001b[39mbase_model_en_de, src\u001b[38;5;241m=\u001b[39mSRC, tgt\u001b[38;5;241m=\u001b[39mTGT, save_dir\u001b[38;5;241m=\u001b[39mtest_folder \u001b[38;5;241m/\u001b[39m model_name)\n\u001b[1;32m      6\u001b[0m output \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mrun_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 5\u001b[0m, in \u001b[0;36mrun_command\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_command\u001b[39m(args: List[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[1;32m      3\u001b[0m         args, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m proc:\n\u001b[0;32m----> 5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proc\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate test results from news_dataset\n",
    "\n",
    "model_name = f\"big_de_en_{news_dataset}\"\n",
    "\n",
    "args = generate_args(data_dir / news_dataset / f\"bin-{SRC}-{TGT}\", \"test\", model_checkpoint=base_model_en_de, src=SRC, tgt=TGT, save_dir=test_folder / model_name)\n",
    "output = args.rsplit(\">\", 1)[-1].strip()\n",
    "print(args)\n",
    "run_command(args)\n",
    "\n",
    "evaluate_news = process_outputs(output)\n",
    "hyp_args, ref_args, src_args = evaluate_news.values()\n",
    "print(hyp_args, ref_args, src_args, sep='\\n')\n",
    "\n",
    "run_command(hyp_args)\n",
    "run_command(ref_args)\n",
    "run_command(src_args)\n",
    "\n",
    "output = Path(output)\n",
    "args, output = get_bleu_score(output)\n",
    "print(args)\n",
    "run_command(args)\n",
    "print_file(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairseq-preprocess --source-lang de --target-lang en --srcdict /Users/Matey/project/nlp2/Models/dict.de.txt --tgtdict /Users/Matey/project/nlp2/Models/dict.en.txt --trainpref /Users/Matey/project/nlp2/Data/it-parallel/train.tok.spm --validpref /Users/Matey/project/nlp2/Data/it-parallel/dev.tok.spm --testpref /Users/Matey/project/nlp2/Data/it-parallel/test.tok.spm --destdir /Users/Matey/project/nlp2/Data/it-parallel/bin-de-en --thresholdtgt 0 --thresholdsrc 0 --workers 20\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "\n",
      "INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', run_sanity_validation_steps=False, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref=None, validpref=None, testpref=None, align_suffix=None, destdir='data-bin', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/Matey/project/nlp2/.venv/bin/fairseq-preprocess\", line 8, in <module>\n",
      "\n",
      "    sys.exit(cli_main())\n",
      "\n",
      "             ^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/Matey/project/nlp2/.venv/lib/python3.11/site-packages/fairseq_cli/preprocess.py\", line 389, in cli_main\n",
      "\n",
      "    main(args)\n",
      "\n",
      "  File \"/Users/Matey/project/nlp2/.venv/lib/python3.11/site-packages/fairseq_cli/preprocess.py\", line 337, in main\n",
      "\n",
      "    assert (\n",
      "\n",
      "AssertionError: --trainpref must be set if --srcdict is not specified\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: 1 args: ['fairseq-preprocess', '--source-lang', 'de', '-...>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = binarize_data(\"de\", \"en\", dict_de, dict_en, train_prefix_file=data_dir / it_parallel / \"train\", valid_prefix_file=data_dir / it_parallel / \"dev\", test_prefix_file=data_dir / it_parallel / \"test\",  output_dir=data_dir / it_parallel / \"bin-de-en\")\n",
    "print(shlex.join(args))\n",
    "run_command(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', run_sanity_validation_steps=False, criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='de', target_lang='en', trainpref='/Users/Matey/project/nlp2/Data/it-parallel/train.tok.spm', validpref='/Users/Matey/project/nlp2/Data/it-parallel/dev.tok.spm', testpref='/Users/Matey/project/nlp2/Data/it-parallel/test.tok.spm', align_suffix=None, destdir='/Users/Matey/project/nlp2/Data/it-parallel/bin-de-en', thresholdtgt=0, thresholdsrc=0, tgtdict='/Users/Matey/project/nlp2/Models/dict.en.txt', srcdict='/Users/Matey/project/nlp2/Models/dict.de.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=20, dict_only=False)\n",
      "INFO:fairseq_cli.preprocess:[de] Dictionary: 32016 types\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:[de] /Users/Matey/project/nlp2/Data/it-parallel/train.tok.spm.de: 20000 sents, 429230 tokens, 0.167% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[de] Dictionary: 32016 types\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:[de] /Users/Matey/project/nlp2/Data/it-parallel/dev.tok.spm.de: 2000 sents, 43767 tokens, 0.165% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[de] Dictionary: 32016 types\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:[de] /Users/Matey/project/nlp2/Data/it-parallel/test.tok.spm.de: 2000 sents, 41598 tokens, 0.231% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[en] Dictionary: 32016 types\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:[en] /Users/Matey/project/nlp2/Data/it-parallel/train.tok.spm.en: 20000 sents, 373732 tokens, 0.011% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[en] Dictionary: 32016 types\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:[en] /Users/Matey/project/nlp2/Data/it-parallel/dev.tok.spm.en: 2000 sents, 38263 tokens, 0.00261% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:[en] Dictionary: 32016 types\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "INFO:fairseq_cli.preprocess:[en] /Users/Matey/project/nlp2/Data/it-parallel/test.tok.spm.en: 2000 sents, 36224 tokens, 0.00276% replaced (by <unk>)\n",
      "INFO:fairseq_cli.preprocess:Wrote preprocessed data to /Users/Matey/project/nlp2/Data/it-parallel/bin-de-en\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess --source-lang de --target-lang en --srcdict /Users/Matey/project/nlp2/Models/dict.de.txt --tgtdict /Users/Matey/project/nlp2/Models/dict.en.txt --trainpref /Users/Matey/project/nlp2/Data/it-parallel/train.tok.spm --validpref /Users/Matey/project/nlp2/Data/it-parallel/dev.tok.spm --testpref /Users/Matey/project/nlp2/Data/it-parallel/test.tok.spm --destdir /Users/Matey/project/nlp2/Data/it-parallel/bin-de-en --thresholdtgt 0 --thresholdsrc 0 --workers 20\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
