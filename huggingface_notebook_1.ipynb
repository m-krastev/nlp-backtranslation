{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akDZZDOpkqtF"
      },
      "source": [
        "# The Backtranslation Notebook to Rule Over Them All\n",
        "Please do ignore the title, I just wanted to make it sound cool. This notebook is a simple demonstration of how to use backtranslation to improve the performance of a machine learning model. The notebook is divided into the following sections:\n",
        "1. Introduction and Simple Generation\n",
        "2. Applying Data Preparation/Filtering\n",
        "3. Investigating Iterative Backtranslation\n",
        "4. Applying a similar pipeline to a more complex model (ALMA-R)\n",
        "5. Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Navigate to your project directory on Google Drive\n",
        "# import os\n",
        "# project_dir = '/content/drive/MyDrive/nlp-backtranslation'\n",
        "# os.chdir(project_dir)\n",
        "\n",
        "# # Install required packages\n",
        "# !pip install transformers\n",
        "# !pip install pytorch-lightning\n",
        "# !pip install peft\n",
        "# !pip install evaluate\n",
        "# !pip install sentencepiece\n",
        "# !pip install -U sacremoses\n",
        "# !pip install sacrebleu\n",
        "# !pip install unbabel-comet"
      ],
      "metadata": {
        "id": "AB8V1QkAl2h1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Na2S553hkqtH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9d7f7a-60ae-4b19-b629-f8ccd19c58d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# First cell in the notebook to enable autoreload of modules\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from calculate_bleu import calculate_bleu, calculate_comet"
      ],
      "metadata": {
        "id": "5Yf8iyQoZkA6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLc7lSyrkqtH",
        "outputId": "75452753-0234-4517-9d60-98c1be680a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-de-en and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,179,648 || all params: 273,027,072 || trainable%: 0.4321\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
        "from utils.data import TranslationDataModule\n",
        "from utils.models import TranslationLightning\n",
        "from pytorch_lightning import Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "# Isn't it so nice and clean now? I went through FOUR different ways of doing this before I thought of this one. WDWFDGFEQWDQWFGA\n",
        "\n",
        "SRC = \"de\"\n",
        "TGT = \"en\"\n",
        "BATCH_SIZE = 16\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "cwd = Path.cwd()\n",
        "data_dir = cwd / \"Data\"\n",
        "\n",
        "it_parallel = \"it-parallel\"\n",
        "news_dataset = \"train-euro-news-big\"\n",
        "it_mono = \"it-mono\"\n",
        "generation_folder = \"generation+it-parallel-en-de\"\n",
        "\n",
        "test_folder = cwd / \"tests\"\n",
        "output_folder = test_folder / f\"generation-{TGT}-{SRC}\"\n",
        "\n",
        "mname = f\"facebook/wmt19-{SRC}-{TGT}\"\n",
        "tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
        "model = FSMTForConditionalGeneration.from_pretrained(mname)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r = 16,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.2,\n",
        "    target_modules = [\"v_proj\", \"q_proj\"]\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath='/content/drive/MyDrive/checkpoints',\n",
        "    save_top_k=1,\n",
        "    monitor='val_loss',\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# it_parallel_data = TranslationDataModule(data_dir / it_parallel, SRC, TGT, tokenizer, None, batch_size=BATCH_SIZE, max_length=MAX_LENGTH)\n",
        "# # Add news_dataset\n",
        "# combined_data_module = TranslationDataModule(data_dir,\n",
        "#                                              SRC,\n",
        "#                                              TGT,\n",
        "#                                              tokenizer,\n",
        "#                                              None,\n",
        "#                                              batch_size=BATCH_SIZE,\n",
        "#                                              max_length=MAX_LENGTH,\n",
        "#                                              use_combined_data=True,\n",
        "#                                              generation_folder=generation_folder)\n",
        "# combined_data_module.setup('fit')\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "#model_pl = TranslationLightning(model, tokenizer, lr=3e-4, adam_beta=(0.9, 0.98), weight_decay=1e-4, output_dir = output_folder)\n",
        "\n",
        "trainer = Trainer(max_epochs=1, check_val_every_n_epoch=5, gradient_clip_val=0.3, val_check_interval = 0.25, limit_val_batches=0.25, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from evaluate import load\n",
        "from statistics import stdev\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
        "from pytorch_lightning import Trainer\n",
        "from utils.data import TranslationDataModule\n",
        "from utils.models import TranslationLightning\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Define constants\n",
        "SRC = \"de\"\n",
        "TGT = \"en\"\n",
        "BATCH_SIZE = 16\n",
        "MAX_LENGTH = 128\n",
        "cwd = Path.cwd()\n",
        "data_dir = cwd / \"Data\"\n",
        "generation_folder = \"generation+it-parallel-en-de\"\n",
        "output_folder_base = cwd / \"results\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "mname = f\"facebook/wmt19-{SRC}-{TGT}\"\n",
        "tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
        "model = FSMTForConditionalGeneration.from_pretrained(mname)\n",
        "config = LoraConfig(r=16, lora_alpha=16, lora_dropout=0.2, target_modules=[\"v_proj\", \"q_proj\"])\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "def calculate_bleu(hyps: list[str], refs: list[str]):\n",
        "    sacrebleu = load(\"sacrebleu\")\n",
        "    bleu = sacrebleu.compute(predictions=hyps, references=refs)\n",
        "    return bleu\n",
        "\n",
        "def calculate_comet(hyps: list[str], refs: list[str], src: list[str]):\n",
        "    comet = load(\"comet\")\n",
        "    score = comet.compute(predictions=hyps, references=refs, sources=src)\n",
        "    score = f\"COMET score: {score['mean_score']:.3f}Â±{stdev(score['scores']):.3f}\"\n",
        "    return score\n",
        "\n",
        "def train_and_evaluate(top_percentage=None, log_file='training_log.csv'):\n",
        "    output_folder = output_folder_base / f\"{top_percentage}_percent\"\n",
        "    output_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    combined_data_module = TranslationDataModule(\n",
        "        data_dir,\n",
        "        SRC,\n",
        "        TGT,\n",
        "        tokenizer,\n",
        "        None,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        max_length=MAX_LENGTH,\n",
        "        use_combined_data=(top_percentage is not None),\n",
        "        generation_folder=generation_folder,\n",
        "        top_percentage=top_percentage if top_percentage else 1.0\n",
        "    )\n",
        "    combined_data_module.setup('fit')\n",
        "\n",
        "    model_pl = TranslationLightning(model, tokenizer, lr=3e-4, adam_beta=(0.9, 0.98), weight_decay=1e-4, output_dir=output_folder)\n",
        "\n",
        "    # Check the first few samples of the train dataset\n",
        "    for i in range(5):\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(\"Source:\", combined_data_module.train.src_texts[i])\n",
        "        print(\"Target:\", combined_data_module.train.tgt_texts[i])\n",
        "\n",
        "    # Check the dataset sizes\n",
        "    print(\"Train dataset size:\", len(combined_data_module.train))\n",
        "    print(\"Validation dataset size:\", len(combined_data_module.val))\n",
        "    print(\"Test dataset size:\", len(combined_data_module.test))\n",
        "\n",
        "    # Setup the trainer\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=output_folder,\n",
        "        filename=\"{epoch}-{val_loss:.2f}\",\n",
        "        monitor=\"val_loss\",\n",
        "        save_top_k=1,\n",
        "        mode=\"min\",\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        max_epochs=1,\n",
        "        check_val_every_n_epoch=1,\n",
        "        gradient_clip_val=0.3,\n",
        "        val_check_interval=0.25,\n",
        "        limit_val_batches=0.25,\n",
        "        callbacks=[checkpoint_callback]\n",
        "    )\n",
        "\n",
        "    # Train and evaluate\n",
        "    trainer.fit(model_pl, datamodule=combined_data_module)\n",
        "    results = trainer.predict(model_pl, datamodule=combined_data_module)\n",
        "    bleu = sum(results) / len(results)\n",
        "    print(\"Average BLEU:\", bleu)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    with open(output_folder / \"hypothesis.hyp\", \"r\") as hyp_file:\n",
        "        hyps = hyp_file.readlines()\n",
        "    with open(output_folder / \"reference.ref\", \"r\") as ref_file:\n",
        "        refs = ref_file.readlines()\n",
        "    bleu_score = calculate_bleu(hyps, refs)\n",
        "    print(\"BLEU score:\", bleu_score)\n",
        "\n",
        "    # Calculate COMET score if source file exists\n",
        "    src_file = output_folder / \"source.src\"\n",
        "    if src_file.exists():\n",
        "        with open(src_file, \"r\") as src_file:\n",
        "            src = src_file.readlines()\n",
        "        comet_score = calculate_comet(hyps, refs, src)\n",
        "        print(\"COMET score:\", comet_score)\n",
        "\n",
        "    # Log results to CSV\n",
        "    log_data = {\n",
        "        'top_percentage': top_percentage,\n",
        "        'train_loss': model_pl.trainer.logged_metrics.get('train_loss', None),\n",
        "        'val_loss': model_pl.trainer.logged_metrics.get('val_loss', None),\n",
        "        'val_bleu': model_pl.trainer.logged_metrics.get('val_bleu', None),\n",
        "        'average_bleu': bleu,\n",
        "        'bleu_score': bleu_score['score']\n",
        "    }\n",
        "    if 'comet_score' in locals():\n",
        "        log_data['comet_score'] = comet_score\n",
        "\n",
        "    df = pd.DataFrame([log_data])\n",
        "    if not Path(log_file).exists():\n",
        "        df.to_csv(log_file, index=False)\n",
        "    else:\n",
        "        df.to_csv(log_file, mode='a', header=False, index=False)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6Vf5l7KUZiH",
        "outputId": "9631509b-fe9a-43a6-c548-8959324177ab"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-de-en and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,179,648 || all params: 273,027,072 || trainable%: 0.4321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate with 50% data\n",
        "print(\"Training with 50% data:\")\n",
        "train_and_evaluate(top_percentage=0.5, log_file='training_log.csv')"
      ],
      "metadata": {
        "id": "toHCa58zxFMZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate with 30% data\n",
        "print(\"Training with 30% data:\")\n",
        "train_and_evaluate(top_percentage=0.3, log_file='training_log.csv')"
      ],
      "metadata": {
        "id": "cERWiAa56gLw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate with 70% data\n",
        "print(\"Training with 70% data:\")\n",
        "train_and_evaluate(top_percentage=0.7, log_file='training_log.csv')"
      ],
      "metadata": {
        "id": "c53wn2DzxQwp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ablation study: Train and evaluate without using the metric (use all data)\n",
        "print(\"Ablation study: Using all data without applying the metric:\")\n",
        "train_and_evaluate(top_percentage=None, log_file='training_log.csv')"
      ],
      "metadata": {
        "id": "l5hXig876esL"
      },
      "execution_count": 37,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}