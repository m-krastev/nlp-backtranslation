#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=1

#SBATCH --job-name=NLP2-FT-PARA
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --time=00:59:00
#SBATCH --mem=60000M
#SBATCH --output=/home/mkrastev/nlp2/%A.out

date

export HF_DATASETS_CACHE=/scratch-local/mkrastev/hf_cache_dir

WORK_DIR=$HOME/nlp2
cd $WORK_DIR

module load 2023
module load PyTorch/2.1.2-foss-2023a-CUDA-12.1.1
source $HOME/FoMo-LoRA/.venv/bin/activate


export WANDB_API_KEY=6f8979c59a86a5404f5ebdc6637e8f1087e36fef
pip install -q wandb
wandb login 6f8979c59a86a5404f5ebdc6637e8f1087e36fef


TOKENIZERS_PARALLELISM=false


srun python train.py \
    --output_dir tests/bt-generation-en-de \
    --batch_size 16 \
    --max_length 128 \
    --epochs 10 \
    --train_dir generation-en-de