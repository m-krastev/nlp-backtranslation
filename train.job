#!/bin/bash

#SBATCH --partition=gpu
#SBATCH --gpus=1

#SBATCH --job-name=NLP2-FT-PARA
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --time=02:59:00
#SBATCH --mem=32G
#SBATCH --output=/home/mkrastev/nlp2/%A.out

date

export HF_DATASETS_CACHE=/scratch-local/mkrastev/hf_cache_dir

WORK_DIR=$HOME/nlp2
cd $WORK_DIR

module load 2023
module load PyTorch/2.1.2-foss-2023a-CUDA-12.1.1
source $HOME/FoMo-LoRA/.venv/bin/activate


export WANDB_API_KEY=6f8979c59a86a5404f5ebdc6637e8f1087e36fef
wandb login 6f8979c59a86a5404f5ebdc6637e8f1087e36fef


TOKENIZERS_PARALLELISM=false


# srun python generate.py \
#     --output_dir tests/iterated-generation-de-en \
#     --srclang de --tgtlang en \
#     --batch_size 32 \
#     --max_length 128 \
#     --data_dir Data/it-mono --split train

srun python train.py \
    --data_dir Data/-generation-de-en \
    --output_dir tests/bt-iterated-de-en-it-parallel \
    --srclang de --tgtlang en \
    --batch_size 32 \
    --max_length 128 \
    --check_val_every_n_epoch 5 \
    --epochs 15