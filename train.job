#!/bin/bash

#SBATCH --partition=gpu_mig
#SBATCH --gpus=1

#SBATCH --job-name=NLP2-FT-PARA
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --time=00:59:00
#SBATCH --mem=32G
#SBATCH --output=/home/mkrastev/nlp2/%A.out

date

export HF_DATASETS_CACHE=/scratch-local/mkrastev/hf_cache_dir

WORK_DIR=$HOME/nlp2
cd $WORK_DIR

module load 2023
module load PyTorch/2.1.2-foss-2023a-CUDA-12.1.1
source $HOME/FoMo-LoRA/.venv/bin/activate


export WANDB_API_KEY=6f8979c59a86a5404f5ebdc6637e8f1087e36fef
wandb login 6f8979c59a86a5404f5ebdc6637e8f1087e36fef
TOKENIZERS_PARALLELISM=false

data_dir=Data/iterated-generation-de-en
out_dir=tests/deepl-de-en-it-parallel

# srun python generate.py \
#     --output_dir $out_dir \
#     --srclang de --tgtlang en \
#     --batch_size 32 \
#     --max_length 128 \
#     --load_from_checkpoint /home/mkrastev/nlp2/Models/iterated-it-mono/checkpoints/epoch=14-step=46875.ckpt \
#     --data_dir $data_dir --split test
# python calculate_bleu.py $out_dir/hypothesis.hyp $out_dir/reference.ref --src $out_dir/source.src --comet

srun python train.py \
    --data_dir Data/generation+it-parallel-en-de \
    --output_dir tests/generation-de-en-0.7 \
    --srclang de --tgtlang en \
    --batch_size 32 \
    --max_length 128 \
    --check_val_every_n_epoch 5 \
    --use_diversity_metric --top_percentage 0.7 \
    --only_predict \
    --load_from_checkpoint /home/mkrastev/nlp2/huggingface/9e864n2x/checkpoints/epoch=4-step=10940.ckpt \
    --epochs 10

python calculate_bleu.py tests/generation-de-en-0.7/hypothesis.hyp tests/generation-de-en-0.7/reference.ref --src tests/generation-de-en-0.7/source.src --comet
