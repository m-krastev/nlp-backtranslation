#!/bin/bash

#SBATCH --partition=gpu_mig
#SBATCH --gpus=1

#SBATCH --job-name=NLP2-FT-PARA
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --time=00:59:00
#SBATCH --mem=32G
#SBATCH --output=/home/mkrastev/nlp2/%A.out

date

export HF_DATASETS_CACHE=/scratch-local/mkrastev/hf_cache_dir

WORK_DIR=$HOME/nlp2
cd $WORK_DIR

module load 2023
module load PyTorch/2.1.2-foss-2023a-CUDA-12.1.1
source $HOME/FoMo-LoRA/.venv/bin/activate


export WANDB_API_KEY=6f8979c59a86a5404f5ebdc6637e8f1087e36fef
wandb login 6f8979c59a86a5404f5ebdc6637e8f1087e36fef
TOKENIZERS_PARALLELISM=false

data_dir=Data/iterated-generation-de-en
out_dir=tests/deepl-de-en-it-parallel

# srun python generate.py \
#     --output_dir $out_dir \
#     --srclang de --tgtlang en \
#     --batch_size 32 \
#     --max_length 128 \
#     --load_from_checkpoint /home/mkrastev/nlp2/Models/iterated-it-mono/checkpoints/epoch=14-step=46875.ckpt \
#     --data_dir $data_dir --split test
python calculate_bleu.py $out_dir/hypothesis.hyp $out_dir/reference.ref --src $out_dir/source.src --comet

# srun python generate.py \
#     --output_dir tests/deepl-de-en-euro-news \
#     --srclang de --tgtlang en \
#     --batch_size 32 \
#     --max_length 128 \
#     --load_from_checkpoint /home/mkrastev/nlp2/Models/deepl/checkpoints/epoch=14-step=18765.ckpt \
#     --data_dir Data/raw-euro-news --split test
# srun python calculate_bleu.py tests/deepl-de-en-it-parallel/hypothesis.hyp tests/deepl-de-en-it-parallel/reference.ref --src tests/deepl-de-en-it-parallel/source.src --comet

# srun python train.py \
#     --data_dir Data/deepl-it-mono-de-en \
#     --output_dir tests/deepl-de-en-it-parallel \
#     --srclang de --tgtlang en \
#     --batch_size 32 \
#     --max_length 128 \
#     --check_val_every_n_epoch 5 \
#     --epochs 15